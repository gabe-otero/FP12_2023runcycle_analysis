{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319045a4-edf5-4a62-b401-2218c7679be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\LANL\\analysis\n",
      "D:\\LANLLa_sample/\n",
      "processing data: La_sample/run11139\n",
      "saving to state & norm information to SF_Norm_files/La_sample/11139\n",
      "0.12203645706176758\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import uproot\n",
    "import pandas as pd\n",
    "import math\n",
    "import statistics as st\n",
    "import os\n",
    "from numba import jit\n",
    "import time\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir('D:/LANL/')\n",
    "datadir = 'sample_data/'\n",
    "folder = 'La_sample/'\n",
    "run_num = \"11139\" \n",
    "print(os.getcwd() + folder)\n",
    "statefileloc = 'D:\\LANL\\SF_Norm_files\\TR_R_expected_avgs_stds_afterclip.csv'\n",
    "savefilename = 'SF_Norm_files/'+folder+run_num\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "start = time.time()\n",
    "fullstart = time.time()\n",
    "\n",
    "chan_enab = [25,26,27,28]\n",
    "\n",
    "read_data = []\n",
    "fileLength = []\n",
    "\n",
    "for el in chan_enab:\n",
    "    f = open(datadir + folder + 'run' + run_num + \"_ch\" + str(el) + \".bin\", 'rb')\n",
    "    read_data.append(np.fromfile(file=f, dtype=np.uint16))\n",
    "    f.close()\n",
    "    fileLength.append(len(read_data[-1]))\n",
    "\n",
    "fileLength = np.asarray(fileLength)\n",
    "# read_data = np.asarray(read_data, dtype = object)  ## cannot do np array for sorting because He and SF are different sizes\n",
    "\n",
    "print('processing data: ' + folder + 'run' + run_num)\n",
    "print('saving to state & norm information to ' + savefilename)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "# print(read_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff448e-fd59-42e0-b8b2-adb846e9318c",
   "metadata": {},
   "source": [
    "Store the big header for each channel in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc0522c-582d-4057-b357-70512dd0fcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target is La\n",
      "Foil is empty\n",
      "Shutter is open: True\n",
      "Facility t0 is on: True\n",
      "Spin flipper is on: True\n",
      "Spin filter is on: True\n",
      "Target is present: True\n",
      "Foil is present: False\n"
     ]
    }
   ],
   "source": [
    "BoardID = []\n",
    "recordLength = []\n",
    "numSamples = []\n",
    "eventCounter = []\n",
    "decFactor = []\n",
    "chanDec = []\n",
    "postTrig = []\n",
    "groupStart = []\n",
    "groupEnd = []\n",
    "timestamp= []\n",
    "sizeFirstEvent = []\n",
    "TTT = []\n",
    "\n",
    "targetDict = {0: \"La\", 1: \"Tb2O3\", 2: \"Yb2O3\", 3: \"Sm2O3\", 4: \"Er2O3\", 5: \"Ho2O3\", 6: \"other\"}\n",
    "foilDict = {0: \"TBD\", 1: \"TBD\", 2: \"TBD\", 3: \"TBD\", 4: \"TBD\", 5: \"TBD\", 6: \"other\"}\n",
    "\n",
    "target=(read_data[0][5]&0x00F0)>>4\n",
    "foil=read_data[0][5]&0x000F\n",
    "targetFlag = read_data[0][5]>>8&1\n",
    "foilFlag = read_data[0][5]>>9&1\n",
    "spinFiltFlag = read_data[0][5]>>10&1\n",
    "spinFlipFlag = read_data[0][5]>>11&1\n",
    "shutterFlag = read_data[0][5]>>12&1\n",
    "facilityTrigFlag = read_data[0][5]>>13&1\n",
    "\n",
    "if targetFlag:\n",
    "    target=targetDict[(read_data[0][5]&0x00F0)>>4]\n",
    "    \n",
    "else:\n",
    "    target = \"empty\"\n",
    "    \n",
    "    \n",
    "if foilFlag:\n",
    "    foil=foilDict[read_data[0][5]&0x000F]\n",
    "else:\n",
    "    foil = \"empty\"\n",
    "\n",
    "for i in range(0,len(chan_enab)):\n",
    "    BoardID.append(read_data[i][9]>>8)\n",
    "    recordLength.append(((read_data[i][9]&0x00FF)<<16)+read_data[i][8])\n",
    "    numSamples.append(((read_data[i][11]&0x00FF)<<16)+read_data[i][10])\n",
    "    eventCounter.append(read_data[i][6]+(read_data[i][7]<<16))\n",
    "    BoardID.append(read_data[i][9]>>8)  \n",
    "    decFactor.append(read_data[i][11]>>8)\n",
    "    chanDec.append(read_data[i][13]>>8)\n",
    "    postTrig.append(read_data[i][15]>>8)\n",
    "    groupStart.append(((read_data[i][13]&0x00FF)<<16)+read_data[i][12])\n",
    "    groupEnd.append(((read_data[i][15]&0x00FF)<<16)+read_data[i][14])\n",
    "    \n",
    "    timestamp.append(read_data[i][16]+(read_data[i][17]<<16)+(read_data[i][18]<<32)+(read_data[i][19]<<40))  \n",
    "    sizeFirstEvent.append(read_data[i][0]+(read_data[i][1]<<16))\n",
    "    TTT.append(read_data[i][2]+(read_data[i][3]<<16)+(read_data[i][4]<<32))\n",
    "    \n",
    "#     print(\"For channel \" + str(chan_enab[i]) + \", BoardID is \" + str(BoardID[i])\n",
    "#           + \"; record length is \" + str(recordLength[i]) + \"; num Samples is \" \n",
    "#           + str(numSamples[i]) + \"; event counter is \" + str(eventCounter[i]) + \"; dec factor is \" + str(decFactor[i]) + \"; chan dec is \" \n",
    "#           + str(chanDec[i]) + \"; postTrig is \" + str(postTrig[i]) + \"; group start is \" + str(groupStart[i]) + \"; group end is \" + str(groupEnd[i])\n",
    "#           + \"; epoch time is \" + str(timestamp[i]) +  \"; first event size is \" + str(sizeFirstEvent[i]) + \"; and ETTT is \" + str(TTT[i]) + \"\\n\")\n",
    "\n",
    "numSamples = np.asarray(numSamples)\n",
    "\n",
    "eventCounter = np.asarray(eventCounter)\n",
    "TTT = np.asarray(TTT)\n",
    "\n",
    "print(\"Target is \" + target)\n",
    "print(\"Foil is \" + foil)\n",
    "print(\"Shutter is open: \" + str(bool(shutterFlag)))\n",
    "print(\"Facility t0 is on: \" + str(bool(facilityTrigFlag)))\n",
    "print(\"Spin flipper is on: \" + str(bool(spinFlipFlag)))\n",
    "print(\"Spin filter is on: \" + str(bool(spinFiltFlag)))\n",
    "print(\"Target is present: \" + str(bool(targetFlag)))\n",
    "print(\"Foil is present: \" + str(bool(foilFlag)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9958e3-eb29-4529-b2bf-eaff422b6419",
   "metadata": {},
   "source": [
    "Determine the time axis for each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762903b1-1b4a-43a9-bd31-137f31cb6168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preTime = []\n",
    "startTime = []\n",
    "endTime = []\n",
    "resolution = []\n",
    "xs = [] \n",
    "\n",
    "for i in range(0,len(chan_enab)):\n",
    "    preTime.append((100-postTrig[i])*recordLength[i]/100)\n",
    "    startTime.append((-1*preTime[i]*16*decFactor[i] + groupStart[i]*16*decFactor[i]))\n",
    "    endTime.append((-1*preTime[i]*16*decFactor[i] + groupEnd[i]*16*decFactor[i]))\n",
    "    resolution.append(16*chanDec[i]*decFactor[i])\n",
    "#     print(\"Pretime for channel\", chan_enab[i],\"is \" + str(preTime[i]) + \"; start time is \" + str(startTime[i]) + \"; end time is \" + str(endTime[i]) \n",
    "#           + \"; resolution is \" + str(resolution[i]) + \"ns\")\n",
    "    xs.append(np.arange(startTime[i],(numSamples[i])*resolution[i]+startTime[i], resolution[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de7d97c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataread from binary time: 1.1029026508331299\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "if chan_enab[0] != 25:\n",
    "    #print('No 3He. Cannot normalize')\n",
    "    raise Exception('3He not in first channel loaded. Cannot normalize')\n",
    "else:\n",
    "    pass\n",
    "\n",
    "@jit(nopython = True)\n",
    "def dataread(data, channels, fileLen, numSamps):\n",
    "    numRuns = int((fileLen[0]-20-numSamps[0])/(numSamps[0]+6)+1)\n",
    "    ys_arr = np.zeros((len(channels), numRuns,numSamps[0]), dtype=np.uint16)\n",
    "    ETTT_arr = np.zeros((len(channels), numRuns), dtype=np.intc)\n",
    "    eventcount_arr = np.zeros((len(channels), numRuns), dtype=np.intc)\n",
    "    for i in range(0,len(channels)):\n",
    "        eventCount = 0\n",
    "        byteCounter = 0\n",
    "            #byte counter is really 2bytecounter, lol\n",
    "        while byteCounter < fileLen[i]:\n",
    "            if byteCounter == 0:\n",
    "                ETTT_arr[i]=TTT[i]\n",
    "                #ETTT_arr[i].append(TTT[i])\n",
    "                eventcount_arr[i]=(eventCounter[i])\n",
    "                byteCounter = 20\n",
    "            else:\n",
    "                ETTT_arr[i]=(data[i][byteCounter]+(data[i][byteCounter+1]<<16)+(data[i][byteCounter+2]<<32))\n",
    "                eventcount_arr[i]=(data[i][byteCounter+4]+(data[i][byteCounter+5]<<16))\n",
    "                byteCounter += 6\n",
    "            for j in range(0, numSamps[i]):\n",
    "                ys_arr[i][eventCount][j]=data[i][byteCounter]\n",
    "                byteCounter += 1\n",
    "            eventCount += 1\n",
    "    return ys_arr, ETTT_arr, eventcount_arr\n",
    "\n",
    "ys_arrHe, ETTT_arrHe, eventcount_arrHe  = dataread(read_data, np.array([25]), fileLength, numSamples) ##hardcoded channel 25 for He\n",
    "ys_arr, ETTT_arr, eventcount_arr  = dataread(read_data[1:], np.array([26,27,28]), fileLength[1:], numSamples[1:]) ##hardcoded channels for coils\n",
    "\n",
    "ETTT_arr = np.vstack([ETTT_arrHe,ETTT_arr]) ## ordering makes sure that first array of new ETTT_arr is ETTT_arr of He\n",
    "eventcount_arr = np.vstack([eventcount_arrHe,eventcount_arr])\n",
    "\n",
    "end = time.time()\n",
    "print('dataread from binary time: ' + str(end-start))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8276405-f7a7-4ee7-8952-3a1445c14ccb",
   "metadata": {},
   "source": [
    "Put ADC values in arrays for each channel (one array per event, an array of events per channel) and put the miniheader information in an array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec628b4-06e7-4922-8888-6c49ae5a439a",
   "metadata": {},
   "source": [
    "Calculate the time difference between each event within a file - used to check for dropping pulses. It seems that if we make the record window 49.152 ms long, we miss every other pulse (at 20 Hz). This is not that surprising - we presumably will not need a lot of data (or any) with the full 50 ms time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24e8d6f-120a-407a-a0dd-f8ae41dc45dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeDif=[]\n",
    "for i in range(0,len(chan_enab)):\n",
    "    timeDif.append([])\n",
    "    for j in range(len(ETTT_arr[i])-1):\n",
    "        timeDif[i].append((ETTT_arr[i][j+1]-ETTT_arr[i][j])*8)\n",
    "#     print(\"Min time difference for channel\", chan_enab[i], \"is\", min(timeDif[i]), \"ns\")\n",
    "#     print(\"Max time difference for channel\", chan_enab[i], \"is\", max(timeDif[i]), \"ns \\n\")\n",
    "#print(timeDif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "440dcb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting and/or base subtraction time: 2.0195047855377197\n"
     ]
    }
   ],
   "source": [
    "#switchs = [31, 74, 117, 160, 203, 246, 289, 332]\n",
    "#%matplotlib notebook\n",
    "baseL = 0\n",
    "baseRCoil = int(((preTime[1]-groupStart[1])*0.70)/chanDec[1])\n",
    "baseRHe = int(((preTime[0]-groupStart[0])*0.70)/chanDec[0])\n",
    "numRuns = int((fileLength[0]-20-numSamples[0])/(numSamples[0]+6)+1)\n",
    "\n",
    "if numSamples[0] != 45000:\n",
    "    print('He channel wrong size')\n",
    "    raise\n",
    "elif numSamples[1] != 351:\n",
    "    print('Coil channel wrong size')\n",
    "    raise\n",
    "elif numSamples[2] != 351:\n",
    "    print('Coil channel wrong size')\n",
    "    raise\n",
    "    \n",
    "#legend =  ['He']\n",
    "legend =  ['LO', 'TR', 'R']\n",
    "#statesID = ['111', '101', '100', '110','101','110','111','100','111']\n",
    "transitions = ['111->101', '101->100', '100->110', '110->101','101->110','110->111','111->100','100->111']\n",
    "switchpulses = np.arange(255,614,45) ##found these pulses which correspond to states 0-8. Change p to plot them\n",
    "#print(switchpulses)\n",
    "p=0\n",
    "\n",
    "s = switchpulses[p]\n",
    "# s = 255\n",
    "t=s+1\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "## dont know why this is so slow ##\n",
    "def plotter(ys, xs, baseR, numpoints):\n",
    "    tempys_basesub = np.zeros((len(ys), numRuns,numpoints[0]), dtype=float)\n",
    "    for i in range((len(ys))):\n",
    "        #tempys_basesub = []\n",
    "        #tempys_basesub = np.zeros((len(ys), numRuns,numpoints[0]), dtype=float)\n",
    "        #tempsums =[]\n",
    "        for pulse in range((len(eventcount_arr[0]))): ## all have 5000 pulses\n",
    "            tempys_basesub[i][pulse]=np.subtract(ys[i][pulse], np.mean(ys[i][pulse][baseL:baseR]))\n",
    "        for j in range(s, t): ## plot only interested pulses\n",
    "            #ys_basesub.append(ys_arr[i][j] - np.mean(ys_arr[i][j][200:6000]))\n",
    "            #print(sum(ys_basesub[i][j])) \n",
    "            plt.plot(xs[i], tempys_basesub[i][j] , label=legend[i]) #+str(sums[1][j])) ## sums[j] will not work for more than just TR   \n",
    "            plt.axvline(xs[0][baseL], ls = '--')\n",
    "            plt.axvline(xs[0][baseR], ls = '--')\n",
    "            #plt.axvline(xs[0][int(((preTime[0]-groupStart[0])*0.70)/chanDec[0])], ls = '--', c ='m')\n",
    "            plt.axvline(xs[0][baseR+5], ls = '--', c ='r') ## BaseR+5 line marks the beginning of the integral, until the end of samples.\n",
    "#             plt.title('SF state transition' + transitions[p]) \n",
    "#             plt.xlabel(\"time from trigger (ns)\")\n",
    "#             plt.ylabel(\"ADC\")\n",
    "            plt.legend()\n",
    "            \n",
    "#plotter(ys_arrHe, xs, baseRHe,numSamples) ##plot 3He\n",
    "#plotter(ys_arr, xs[1:], baseRCoil, numSamples[1:]) ##plot coils\n",
    "\n",
    "#@jit(nopython = True) ## Actually JIT seems to be slower here!\n",
    "def basesub_sum(ys, baseR, numpoints): ## for coils, could be used for He but below does that\n",
    "    tempys_basesub = np.zeros((len(ys), numRuns,numpoints[0]), dtype=np.float64)\n",
    "    tempsums = np.zeros((len(ys), numRuns), dtype=np.float64)\n",
    "    for i in range((len(ys))):\n",
    "        for pulse in range((len(eventcount_arr[0]))): ## all have 5000 pulses\n",
    "            tempys_basesub[i][pulse]=np.subtract(ys[i][pulse], np.mean(ys[i][pulse][baseL:baseR]))\n",
    "            tempsums[i][pulse] = np.sum(tempys_basesub[i][pulse][baseR+5:-1])\n",
    "    return tempys_basesub, tempsums\n",
    "        \n",
    "ys_basesub, sums = basesub_sum(ys_arr, baseRCoil, numSamples[1:])\n",
    "\n",
    "@jit(nopython = True) ## separate function for He because this checks every point, can take a while\n",
    "def basesub_normHe(ys, baseRegion, intgrRegion):\n",
    "    tempys_basesub = np.zeros((len(ys), numRuns,45000), dtype=np.float64) #hardcode numSamples[0] = 45000\n",
    "    tempsums = np.zeros((len(ys), numRuns), dtype=np.float64)\n",
    "    for i in range((len(ys))): ## i is pretty much always 0 for 3He. Left it general.\n",
    "        for pulse in range((len(eventcount_arr[0]))): ## all have 5000 pulses\n",
    "            for j in range(intgrRegion[0]+1000, intgrRegion[1]): ## checking for saturation in He channel. restrict to slightly smaller range\n",
    "                if ys[i][pulse][j] > 4060:  ## cutoff adc value (real is 4092)\n",
    "                    print(('3He is saturating in normalization region at pulse,point: ' + str(pulse) + ', '+ str(j)))\n",
    "                    raise\n",
    "                else:\n",
    "                    pass\n",
    "            tempys_basesub[i][pulse]=np.subtract(ys[i][pulse], np.mean(ys[i][pulse][baseRegion[0]:baseRegion[1]]))\n",
    "            tempsums[i][pulse] = np.sum(tempys_basesub[i][pulse][intgrRegion[0]:intgrRegion[1]])\n",
    "            ## need to investigate adc saturation point\n",
    "    return tempys_basesub, tempsums\n",
    "\n",
    "baseRHe = int(((preTime[0]-groupStart[0])*0.70)/chanDec[0]) #redefined for clarity\n",
    "\n",
    "HeBaseReg = np.array([0, baseRHe])\n",
    "HeIntgrReg = np.array([baseRHe+700, 15999]) ## hardcoded begin/end region for integral over NaI and 6Li regions\n",
    "ys_basesubHe, HeNorms = basesub_normHe(ys_arrHe, HeBaseReg, HeIntgrReg)\n",
    "## got rid of xs in basesub, don't think we need them as an input 06.10.24\n",
    "\n",
    "end = time.time()\n",
    "print('plotting and/or base subtraction time: ' + str(end-start))            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4e56b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find switches time: 0.017999649047851562\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "statefile = pd.read_csv(statefileloc)\n",
    "transitions = statefile['transition'].to_numpy() ## for some reasons \"averages\" and \"std dev\" need a space before them\n",
    "expectedSumsTR_R = statefile[' averages'].to_numpy()\n",
    "expectedStdsTR_R = statefile[' standard dev'].to_numpy()\n",
    "\n",
    "AllSwitches = []\n",
    "tolerance = 1000 ## see comments below\n",
    "\n",
    "## can't use pre-existing np array because usually one array of unequal length\n",
    "for i in range(len(expectedSumsTR_R)):\n",
    "    diff_arr = np.absolute(np.add(sums[1],sums[2]) - expectedSumsTR_R[i])\n",
    "    found_sums =[]\n",
    "    for j in range(len(diff_arr)):\n",
    "#         if diff_arr[j] < expectedStdsTR_R[i]*3: ## within 3 standard deviations of its respective std\n",
    "## using std didn't work for La. Maybe just get rid of it and use a constant value...\n",
    "        if diff_arr[j] < tolerance: ## this uses a constant \"tolerance\"\n",
    "            found_sums.append(j)\n",
    "    AllSwitches.append(np.array(found_sums))\n",
    "\n",
    "end = time.time()\n",
    "print('find switches time: ' + str(end-start)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3518917e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# pulses correct, end of sequence\n",
      "# pulses correct, end of sequence\n",
      "# pulses correct, end of sequence\n",
      "# pulses correct, end of sequence\n",
      "# pulses correct, end of sequence\n",
      "# pulses correct, end of sequence\n",
      "# pulses correct, end of sequence\n",
      "# pulses correct, end of sequence\n",
      "# pulses correct, end of sequence\n",
      "# pulses correct, end of sequence\n",
      "# pulses correct, end of sequence\n",
      "# pulses correct, end of sequence\n",
      "# pulses correct, end of sequence\n",
      "# pulses correct, end of sequence\n",
      "SF dataframe time: 0.008996725082397461\n"
     ]
    }
   ],
   "source": [
    "## testing sorting with pandas dataframe\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "transitions = ['111->101', '101->100', '100->110', '110->101','101->110','110->111','111->100','100->111']\n",
    "\n",
    "transitionSumsTR = []\n",
    "transitionSumsR = []\n",
    "transitionTR_RAvgs = []\n",
    "transitionTR_Rstds = []\n",
    "transitionSumsTR_R = []\n",
    "\n",
    "for i in range(0,len(transitions)):\n",
    "    tempTR = []\n",
    "    tempR = []\n",
    "    for j in range(0,len(AllSwitches[i])):\n",
    "        tempTR.append(sums[1][AllSwitches[i][j]])\n",
    "        tempR.append(sums[2][AllSwitches[i][j]])\n",
    "    transitionSumsTR.append(tempTR)\n",
    "    transitionSumsR.append(tempR)\n",
    "    transitionSumsTR_R.append(np.add(tempTR,tempR))\n",
    "    transitionTR_RAvgs.append(np.average(np.add(tempTR,tempR)))\n",
    "    transitionTR_Rstds.append(np.std(np.add(tempTR,tempR)))\n",
    "\n",
    "cols = ['transition', 'transition_locations', 'sumsTR_R', 'TR_R_avgs', 'TR_R_stds']\n",
    "transSumsData = [transitions, AllSwitches, transitionSumsTR_R, transitionTR_RAvgs, transitionTR_Rstds]\n",
    "\n",
    "df_SF = pd.DataFrame({cols[0]: transSumsData[0],            \n",
    "                   cols[1]: transSumsData[1],\n",
    "                   cols[2]: transSumsData[2],\n",
    "                   cols[3]: transSumsData[3],\n",
    "                   cols[4]: transSumsData[4]})\n",
    "## 'original dataframe to work with, 7 transitions and their associated locations and sums:\n",
    "\n",
    "# with pd.option_context('display.max_rows', None,\n",
    "#                       'display.max_columns', None,\n",
    "#                       'display.precision', 3,\n",
    "#                       ):\n",
    "#    print(df_SF.explode(['transition_locations', 'sumsTR_R']))\n",
    "\n",
    "## original dataframe is exploded so that transition_locations and associated sum is unpacked. Indices of state are kept as \"nickname\" \n",
    "## save Averages and Stds for future use')\n",
    "\n",
    "df_SF = df_SF.explode(['transition_locations', 'sumsTR_R']).reset_index().rename(columns={'index' : 'nicknames'}) #turn the 'index' of the exploded df_SF into a column, then reassign indices\n",
    "\n",
    "## now sort by the transition location and rearrange dataframe indices, only matters for looping (??)\n",
    "df_SF = df_SF.sort_values(by=['transition_locations'])\n",
    "df_SF = df_SF.reset_index(drop=True)\n",
    "\n",
    "for ind in df_SF.index[:-1]:\n",
    "#     print('transition: '+ str(df_SF['nicknames'][ind]) + ' location: ' + str(df_SF['transition_locations'][ind]))\n",
    "    if (df_SF['nicknames'][ind+1])-1 != df_SF['nicknames'][ind]: ## if next transition 'nickname' is not next in sequence, failure\n",
    "        if (df_SF['nicknames'][ind+1])-1 == -1: ## special condition for end of sequence where (0-1) != 7\n",
    "            if (df_SF['transition_locations'][ind+1]-df_SF['transition_locations'][ind]) != 45: ## changed to 45 pulses!\n",
    "                raise Exception('# pulses error: ' + str(df_SF['transition_locations'][ind+1]-df_SF['transition_locations'][ind]))\n",
    "            if (df_SF['transition_locations'][ind+1]-df_SF['transition_locations'][ind]) == 45:\n",
    "                pass\n",
    "                print('# pulses correct, end of sequence')\n",
    "        else:\n",
    "            ## checks that the sequence follows 0-> 1-> 2-> 3... etc order\n",
    "            raise Exception('sorting failure, ' + str((df_SF['nicknames'][ind+1])-1) + '!=' + str(df_SF['nicknames'][ind]))\n",
    "    elif (df_SF['nicknames'][ind+1])-1 == df_SF['nicknames'][ind]:\n",
    "        if (df_SF['transition_locations'][ind+1]-df_SF['transition_locations'][ind]) != 45: ## error if =/= 45 pulses between each\n",
    "            raise Exception('# pulses error: ' + str(df_SF['transition_locations'][ind+1]-df_SF['transition_locations'][ind]))\n",
    "        if (df_SF['transition_locations'][ind+1]-df_SF['transition_locations'][ind]) == 45:\n",
    "            pass\n",
    "    else:\n",
    "        raise Exception('Unknown failure in sorting')\n",
    "        \n",
    "end = time.time()\n",
    "print('SF dataframe time: ' + str(end-start))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41384224",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataframe for He Norms\n",
    "cols = ['pulse', 'norms']\n",
    "pulses = range(numRuns)\n",
    "normsData = [pulses, HeNorms[0]]\n",
    "\n",
    "df_HE = pd.DataFrame({cols[0]: normsData[0],            \n",
    "                   cols[1]: normsData[1]})\n",
    "\n",
    "#print(df_HE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a33c20c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_SF.to_hdf(savefilename + '.h5', f'df_0', mode='w') ## this \"deletes\" any previous data in the file name\n",
    "\n",
    "for idx, df in enumerate([df_SF, df_HE]):\n",
    "    df.to_hdf(savefilename + '.h5', f'df_{idx}', mode='a') # rerunning this without the above 'w' code will keep increasing file size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812393a",
   "metadata": {},
   "source": [
    "## end of file creation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e3de862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full processing time: 3.4507651329040527\n"
     ]
    }
   ],
   "source": [
    "fullend = time.time()\n",
    "print('full processing time: ' + str(fullend-fullstart))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c433649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
