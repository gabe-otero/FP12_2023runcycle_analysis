{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statistics as st\n",
    "import os\n",
    "from numba import njit\n",
    "import time\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\n",
    "import warnings\n",
    "from loguru import logger\n",
    "from datetime import datetime\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings(action='ignore', message='RuntimeWarning: overflow encountered in multiply')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############ real running ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arg in sys.argv:\n",
    "    run_num=str(arg).zfill(5)\n",
    "    # print(run_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_enab = int(sys.argv[-1])\n",
    "run_start=str(sys.argv[1]).zfill(5)\n",
    "run_end=str(sys.argv[2]).zfill(5)\n",
    "run_num=str(sys.argv[3]).zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('F:/LANL/')\n",
    "datadir = 'D:/LANSCE_FP12_2023/data/' ## add directory of hard drive\n",
    "uniquefolder = \"runs\" + str(run_start) + \"-\" + str(run_end) +\"/\"\n",
    "SFNormFile = 'SF_Norm_files/'+uniquefolder+run_num\n",
    "statefileloc = 'F:\\LANL\\SF_Norm_files\\TR_R_expected_avgs_stds_afterclip.csv'\n",
    "processedpolfolder = '/processed_data/'+uniquefolder+'pulseadd_D/'\n",
    "# processedasymfolder = '/processed_data/'+uniquefolder+'asym_D/'\n",
    "# processedasymfolder_bg = '/processed_data/'+uniquefolder+'asym_bg_D/'\n",
    "polSavename = os.getcwd()+processedpolfolder+run_num+'_pulseadd_D'\n",
    "# AsymSavename = os.getcwd()+processedasymfolder+run_num+'_asym_D'\n",
    "# AsymSavename_bg = os.getcwd()+processedasymfolder_bg+run_num+'_asym_bg_D'\n",
    "logger.add(\"F:/LANL/processed_data/\" + uniquefolder + '0_ErrorLog_'+run_start+'_'+run_end+'pulseadd_D.txt', delay = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.getcwd()+processedpolfolder):\n",
    "    # Create the directory\n",
    "    os.makedirs(os.getcwd()+processedpolfolder)\n",
    "    print(\"Directory created successfully\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############ test running ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_num = '12036'<br>\n",
    "os.chdir('F:/LANL/')<br>\n",
    "datadir = 'sample_data/'<br>\n",
    "runs_folder = 'runs12034-12363/'<br>\n",
    "uniquefolder = 'debug_sample/'+runs_folder<br>\n",
    "# uniquefolder = 'La_sample/'<br>\n",
    "# SFNormFile = 'SF_Norm_files/'+uniquefolder+run_num<br>\n",
    "SFNormFile = 'SF_Norm_files/'+runs_folder+run_num<br>\n",
    "# AsymSavename = '****testing testing testing'<br>\n",
    "ONOFFSavename = 'F:/LANL/testing_ONOFF'<br>\n",
    "AsymSavename = 'F:/LANL/testing_asyms'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cannot handle all 24 detectors at once, memory issue... can look into np.empty and deleting variables if needed ##<br>\n",
    "chan_enab = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]) ## all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_enab = np.array([0,1,2,3,4,5,6,7,8,9,10,11,24]) ## downstream _D\n",
    "# chan_enab = np.array([12,13,14,15,16,17,18,19,20,21,22,23,24]) ## upstream _U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(os.getcwd() + folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "fullstart = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data = []\n",
    "fileLength = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file():\n",
    "    for el in chan_enab:\n",
    "        # f = open(datadir + folder + 'run' + run_num + \"_ch\" + str(el) + \".bin\", 'rb')\n",
    "        f = open(datadir+uniquefolder + 'run' + str(run_num) + \"_ch\" +str(el) + \".bin\", 'rb')\n",
    "        read_data.append(np.fromfile(file=f, dtype=np.uint16))\n",
    "        f.close()\n",
    "        fileLength.append(len(read_data[-1]))\n",
    "    return read_data, fileLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileLength = np.asarray(fileLength)\n",
    "read_data = np.asarray(read_data) ## in detector's case, all are the same size samples, so can do read_data as np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if chan_enab[-1] != 24:\n",
    "    emessage = ('last channel is not 6Li detector')\n",
    "    logger.error('run '+run_num + emessage)\n",
    "    raise Exception(emessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('file open time: ' + str(end-start))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('saving processed data to ' + polSavename)\n",
    "print(\"Channel is \" + str(chan_enab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[2]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the big header for each channel in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoardID = []\n",
    "recordLength = []\n",
    "numSamples = []\n",
    "eventCounter = []\n",
    "decFactor = []\n",
    "chanDec = []\n",
    "postTrig = []\n",
    "groupStart = []\n",
    "groupEnd = []\n",
    "timestamp= []\n",
    "sizeFirstEvent = []\n",
    "TTT = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetDict = {0: \"La\", 1: \"Tb2O3\", 2: \"Yb2O3\", 3: \"Sm2O3\", 4: \"Er2O3\", 5: \"Ho2O3\", 6: \"other\"}\n",
    "foilDict = {0: \"TBD\", 1: \"TBD\", 2: \"TBD\", 3: \"TBD\", 4: \"TBD\", 5: \"TBD\", 6: \"other\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=(read_data[0][5]&0x00F0)>>4\n",
    "foil=read_data[0][5]&0x000F\n",
    "targetFlag = read_data[0][5]>>8&1\n",
    "foilFlag = read_data[0][5]>>9&1\n",
    "spinFiltFlag = read_data[0][5]>>10&1\n",
    "spinFlipFlag = read_data[0][5]>>11&1\n",
    "shutterFlag = read_data[0][5]>>12&1\n",
    "facilityTrigFlag = read_data[0][5]>>13&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if targetFlag:\n",
    "    target=targetDict[(read_data[0][5]&0x00F0)>>4]\n",
    "else:\n",
    "    target = \"empty\"\n",
    "if foilFlag:\n",
    "    foil=foilDict[read_data[0][5]&0x000F]\n",
    "else:\n",
    "    foil = \"empty\"\n",
    "for i in range(0,len(chan_enab)):\n",
    "    BoardID.append(read_data[i][9]>>8)\n",
    "    recordLength.append(((read_data[i][9]&0x00FF)<<16)+read_data[i][8])\n",
    "    numSamples.append(((read_data[i][11]&0x00FF)<<16)+read_data[i][10])\n",
    "    eventCounter.append(read_data[i][6]+(read_data[i][7]<<16))\n",
    "    BoardID.append(read_data[i][9]>>8)  \n",
    "    decFactor.append(read_data[i][11]>>8)\n",
    "    chanDec.append(read_data[i][13]>>8)\n",
    "    postTrig.append(read_data[i][15]>>8)\n",
    "    groupStart.append(((read_data[i][13]&0x00FF)<<16)+read_data[i][12])\n",
    "    groupEnd.append(((read_data[i][15]&0x00FF)<<16)+read_data[i][14])\n",
    "    \n",
    "    timestamp.append(read_data[i][16]+(read_data[i][17]<<16)+(read_data[i][18]<<32)+(read_data[i][19]<<40))  \n",
    "    sizeFirstEvent.append(read_data[i][0]+(read_data[i][1]<<16))\n",
    "    TTT.append(read_data[i][2]+(read_data[i][3]<<16)+(read_data[i][4]<<32))\n",
    "    \n",
    "#     print(\"For channel \" + str(chan_enab[i]) + \", BoardID is \" + str(BoardID[i])\n",
    "#           + \"; record length is \" + str(recordLength[i]) + \"; num Samples is \" \n",
    "#           + str(numSamples[i]) + \"; event counter is \" + str(eventCounter[i]) + \"; dec factor is \" + str(decFactor[i]) + \"; chan dec is \" \n",
    "#           + str(chanDec[i]) + \"; postTrig is \" + str(postTrig[i]) + \"; group start is \" + str(groupStart[i]) + \"; group end is \" + str(groupEnd[i])\n",
    "#           + \"; epoch time is \" + str(timestamp[i]) +  \"; first event size is \" + str(sizeFirstEvent[i]) + \"; and ETTT is \" + str(TTT[i]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoardID = np.asarray(BoardID) \n",
    "recordLength = np.asarray(recordLength)\n",
    "numSamples = np.asarray(numSamples)\n",
    "eventCounter = np.asarray(eventCounter)\n",
    "decFactor = np.asarray(decFactor)\n",
    "chanDec = np.asarray(chanDec)\n",
    "postTrig = np.asarray(postTrig)\n",
    "groupStart = np.asarray(groupStart)\n",
    "groupEnd = np.asarray(groupEnd)\n",
    "timestamp = np.asarray(timestamp)\n",
    "sizeFirstEvent = np.asarray(sizeFirstEvent)\n",
    "TTT = np.asarray(TTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target is \" + target)\n",
    "# print(\"Foil is \" + foil)\n",
    "# print(\"Shutter is open: \" + str(bool(shutterFlag)))\n",
    "# print(\"Facility t0 is on: \" + str(bool(facilityTrigFlag)))\n",
    "# print(\"Spin flipper is on: \" + str(bool(spinFlipFlag)))\n",
    "# print(\"Spin filter is on: \" + str(bool(spinFiltFlag)))\n",
    "# print(\"Target is present: \" + str(bool(targetFlag)))\n",
    "# print(\"Foil is present: \" + str(bool(foilFlag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[3]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the time axis for each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preTime = []\n",
    "startTime = []\n",
    "endTime = []\n",
    "resolution = []\n",
    "xs = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(chan_enab)):\n",
    "    preTime.append((100-postTrig[i])*recordLength[i]/100)\n",
    "    startTime.append((-1*preTime[i]*16*decFactor[i] + groupStart[i]*16*decFactor[i]))\n",
    "    endTime.append((-1*preTime[i]*16*decFactor[i] + groupEnd[i]*16*decFactor[i]))\n",
    "    resolution.append(16*chanDec[i]*decFactor[i])\n",
    "#     print(\"Pretime for channel\", chan_enab[i],\"is \" + str(preTime[i]) + \"; start time is \" + str(startTime[i]) + \"; end time is \" + str(endTime[i]) \n",
    "#           + \"; resolution is \" + str(resolution[i]) + \"ns\")\n",
    "    xs.append(np.arange(startTime[i],(numSamples[i])*resolution[i]+startTime[i], resolution[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(preTime)\n",
    "np.asarray(startTime)\n",
    "np.asarray(endTime)\n",
    "np.asarray(resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.asarray(xs) ## can convert xs to np array here because all detectors same numsamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[4]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def dataread(data, channels, fileLen, numSamps):\n",
    "    numRuns = int((fileLen[0]-20-numSamps[0])/(numSamps[0]+6)+1)\n",
    "    ys_arr = np.zeros((len(channels), numRuns,numSamps[0]), dtype=np.uint16)\n",
    "    ETTT_arr = np.zeros((len(channels), numRuns), dtype=np.intc)\n",
    "    eventcount_arr = np.zeros((len(channels), numRuns), dtype=np.intc)\n",
    "    for i in range(0,len(channels)):\n",
    "        eventCount = 0\n",
    "        byteCounter = 0\n",
    "            #byte counter is really 2bytecounter, lol\n",
    "        while byteCounter < fileLen[i]:\n",
    "            if byteCounter == 0:\n",
    "                ETTT_arr[i]=TTT[i]\n",
    "                #ETTT_arr[i].append(TTT[i])\n",
    "                eventcount_arr[i]=(eventCounter[i])\n",
    "                byteCounter = 20\n",
    "            else:\n",
    "                ETTT_arr[i]=(data[i][byteCounter]+(data[i][byteCounter+1]<<16)+(data[i][byteCounter+2]<<32))\n",
    "                eventcount_arr[i]=(data[i][byteCounter+4]+(data[i][byteCounter+5]<<16))\n",
    "                byteCounter += 6\n",
    "            for j in range(0, numSamps[i]):\n",
    "                #if j == 0:\n",
    "                    #ys_arr[i].append([])\n",
    "                #print(byteCounter)\n",
    "                ys_arr[i][eventCount][j]=data[i][byteCounter]\n",
    "                byteCounter += 1\n",
    "            eventCount += 1\n",
    "    return ys_arr, ETTT_arr, eventcount_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "# ys_arrHe, ETTT_arrHe, eventcount_arrHe  = dataread(read_data, [25], fileLength, numSamples) ##hardcoded channel 25 for He\n",
    "ys_arr, ETTT_arr, eventcount_arr  = dataread(read_data, chan_enab, fileLength, numSamples) ##hardcoded channels for coils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print('dataread from binary time: ' + str(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[5]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeDif=[]\n",
    "for i in range(0,len(chan_enab)):\n",
    "    timeDif.append([])\n",
    "    for j in range(len(ETTT_arr[i])-1):\n",
    "        timeDif[i].append((ETTT_arr[i][j+1]-ETTT_arr[i][j])*8)\n",
    "#     print(\"Min time difference for channel\", chan_enab[i], \"is\", min(timeDif[i]), \"ns\")\n",
    "#     print(\"Max time difference for channel\", chan_enab[i], \"is\", max(timeDif[i]), \"ns \\n\")\n",
    "#print(timeDif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[6]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in SF and He normalization information ##<br>\n",
    "SFNormFile2 = 'F:/LANL/SF_Norm_files/runs12034-12363/12036.h5' ## change sf norm file here or use default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_SF = pd.read_hdf(SFNormFile + '.h5', key='df_0')\n",
    "#     df_SF = pd.read_hdf(SFNormFile2, key='df_0')\n",
    "    df_HE = pd.read_hdf(SFNormFile + '.h5', key='df_1')\n",
    "#     df_HE = pd.read_hdf(SFNormFile2, key='df_1')\n",
    "except Exception as e:\n",
    "    logger.error('run '+run_num + ' failed during SFNormFile load')\n",
    "    logger.exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SF_Sort_arr = df_SF[['nicknames', 'transition_locations']].to_numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "He_Norm_arr = df_HE[['pulse', 'norms']].to_numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormFactor = 1000000  ## He integrals are huge, this normalizes all of those by a constant value for ease of use\n",
    "HeNorms= (He_Norm_arr[1])/NormFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(SF_Sort_arr)<br>\n",
    "print(He_Norm_arr[1]/NormFactor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[7]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basesub and plotting ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseL = 0\n",
    "baseR = int(((preTime[0]-groupStart[0])*0.70)/chanDec[0])  ##70% before the trigger\n",
    "numRuns = int((fileLength[0]-20-numSamples[0])/(numSamples[0]+6)+1)\n",
    "legend =  ['NaI', 'R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 20 ## pulse to look at\n",
    "t=s+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " dont know why this is so slow ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(ys, xs, baseR, numpoints):\n",
    "    tempys_basesub = np.zeros((len(ys), numRuns,numpoints[0]), dtype=float)\n",
    "    for i in range((len(ys))):\n",
    "        for pulse in range((len(eventcount_arr[0]))): ## all have 5000 pulses\n",
    "            tempys_basesub[i][pulse]=np.subtract(ys[i][pulse], np.mean(ys[i][pulse][baseL:baseR]))\n",
    "        for j in range(s, t): ## plot only interested pulses\n",
    "            plt.plot(xs[i], tempys_basesub[i][j]) #label=legend[i]) #+str(sums[1][j])) ## sums[j] will not work for more than just TR   \n",
    "            plt.axvline(xs[0][baseL], ls = '--')\n",
    "            plt.axvline(xs[0][baseR], ls = '--')\n",
    "            #plt.axvline(xs[0][int(((preTime[0]-groupStart[0])*0.70)/chanDec[0])], ls = '--', c ='m')\n",
    "            plt.axvline(xs[0][baseR+5], ls = '--', c ='r') ## BaseR+5 line marks the beginning of the integral, until the end of samples.\n",
    "#             plt.title('SF state transition' + transitions[p]) \n",
    "#             plt.xlabel(\"time from trigger (ns)\")\n",
    "#             plt.ylabel(\"ADC\")\n",
    "            plt.legend()\n",
    "            \n",
    "# plotter(ys_arr[9:], xs[9:], baseR, numSamples) ##plot coils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit ## jit is faster for large # channels, slower for small # channels\n",
    "def basesub(ys, baseRight, numpoints): \n",
    "    tempys_basesub = np.zeros((numRuns,numpoints[0]), dtype=np.float64)\n",
    "    for pulse in range((len(eventcount_arr[0]))): ## all have 5000 pulses\n",
    "        tempys_basesub[pulse]=np.subtract(ys[pulse], np.mean(ys[pulse][baseL:baseRight]))\n",
    "    return tempys_basesub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit ## jit is faster for large # channels, slower for small # channels\n",
    "def basesub_norm(ys, baseRight, numpoints): \n",
    "    tempys_basesub = np.zeros((numRuns,numpoints[0]), dtype=np.float64)\n",
    "    for pulse in range((len(eventcount_arr[0]))): ## all have 5000 pulses\n",
    "        tempys_basesub[pulse]=np.subtract(ys[pulse], np.mean(ys[pulse][baseL:baseRight]))\n",
    "        tempys_basesub[pulse]=tempys_basesub[pulse]/HeNorms[pulse] \n",
    "    return tempys_basesub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_basesub = np.zeros((len(ys_arr), numRuns,numSamples[0]), dtype=np.float64)\n",
    "# ys_basesub_norm = np.zeros((len(ys_arr), numRuns,numSamples[0]), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ys_basesub)): ## feeding y arrays into function 1 channel at  a time is faster than all at once\n",
    "    ys_basesub[i] = basesub(ys_arr[i], baseR, numSamples)\n",
    "# for i in range(len(ys_basesub)): ## feeding y arrays into function 1 channel at  a time is faster than all at once\n",
    "#     ys_basesub[i] = basesub_norm(ys_arr[i], baseR, numSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_basesub[-1] = ys_basesub[-1]*-1 ## invert 6Li to positive signal. Comment out if not using\n",
    "# ys_basesub_norm[-1] = ys_basesub_norm[-1]*-1 ## invert 6Li to positive signal. Comment out if not using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "# print('plotting and/or base subtraction time: ' + str(end-start))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[8]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 6Li t0 for all instead of for themselves individually ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaIthresh=2000\n",
    "Li6thresh=400\n",
    "threshold_array = (np.full(len(ys_basesub), NaIthresh))\n",
    "threshold_array[-1] = Li6thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "njit ## numba does not support reversed, but this could be changed if it's slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_offset(ys, thresharr):\n",
    "    xCrosses = np.zeros((len(ys), numRuns)) #outer array is crossing arrays for given channel, inner array is crossing for each event\n",
    "    offset = np.zeros((len(ys), numRuns), dtype=np.int32) ##offset in bins for each channel, each pulse\n",
    "    modeCrosses = np.zeros((len(ys)), dtype=np.float64)\n",
    "    for i in reversed(range(len(ys))):\n",
    "        #xValues.append([])\n",
    "        for p in range(len(ys[i])):\n",
    "            xing = np.argmax(ys[i][p] > thresharr[i])\n",
    "            #print(xing)\n",
    "            xCrosses[i][p] = xing\n",
    "        modeCrosses[i] = (st.mode(xCrosses[i])) #find the most typical crossing value for each channel\n",
    "        for p in range(len(xCrosses[i])):\n",
    "            offset[i][p] = (modeCrosses[-1] - xCrosses[i][p]) ## make sure this is the correct sign!!! \n",
    "    if (np.all(xCrosses[-1])) == False:\n",
    "        emessage = ('ERROR: 6Li threshold was not reached for at least one pulse')\n",
    "        logger.error('run '+run_num + emessage)\n",
    "        raise Exception(emessage)\n",
    "    return offset, xCrosses, modeCrosses\n",
    "                           \n",
    "offset, xCrosses, modeCrosses = find_offset(ys_basesub, threshold_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "# print('finding offset time: ' + str(end-start))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[9]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extend all arrays by a value, check that the max number of offset on 6Li is less than that value ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extendedRange = 3 ## must be a positive value which to extend ys_arr\n",
    "if abs(max(offset[-1], key = abs)) > extendedRange: ## if the max offset of 6Li is >extendedRange, something is wrong\n",
    "    emessage = ('ERROR: largest offset greater than extended range')\n",
    "    logger.error('run '+run_num + emessage)\n",
    "    raise Exception(emessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ys_ext = np.zeros((len(ys_basesub), len(ys_basesub[0]), len(ys_basesub[0][0])+extendedRange*2), dtype=np.float64)\n",
    "    ys_cut = np.zeros((len(ys_basesub), len(ys_basesub[0]), (len(ys_ext[0][0])-((extendedRange*2)+1)*2)))\n",
    "    xs_cut = np.zeros((len(ys_cut), len(ys_cut[0][0])))\n",
    "except Exception as e:\n",
    "    logger.error('run '+run_num + ' failed during ys_cut array creation')\n",
    "    logger.exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cant use jit because np.pad is not supported ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_cut(ys, xs_arr, extendedr):\n",
    "    tempys_ext = np.zeros((len(ys), len(ys[0])+extendedr*2), dtype=np.float64)\n",
    "    tempys_cut = np.zeros((len(ys), (len(tempys_ext[0])-((extendedr*2)+1)*2)))\n",
    "    tempxs_cut = np.zeros(len(tempys_cut[0]))\n",
    "    for p in range(len(ys)):\n",
    "        tempys_ext[p] = np.pad(ys[p], extendedr, 'constant', constant_values=(0))\n",
    "        tempys_ext[p] = np.roll(tempys_ext[p],offset[-1][p]) ## assumes 6Li at -1 position\n",
    "        tempys_cut[p] = tempys_ext[p][((extendedr*2)+1):-((extendedr*2)+1)].copy() ## cut by 7 (if extRange == 3)\n",
    "        tempys_cut[p] = tempys_cut[p]/HeNorms[p] ## normalize by 3He integral  ## comment out if using basesub_norm\n",
    "    x_cut_amt = int((len(ys[0]) - len(tempys_cut[0]))/2)\n",
    "    tempxs_cut = xs_arr[x_cut_amt:-x_cut_amt].copy()\n",
    "    return tempys_cut, tempxs_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looping every channel through function is 5x faster ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in range(len(ys_basesub)):\n",
    "        ys_cut[i], xs_cut[i] = align_cut(ys_basesub[i], xs[i], extendedRange)\n",
    "except Exception as e:\n",
    "    logger.error('run '+run_num + ' failed aligning and cutting')\n",
    "    logger.exception(e)\n",
    "    \n",
    "# checkp = 2053\n",
    "# print(offset[-1][checkp]) ## checking offset for one example checkpulse\n",
    "# print('original index for checkpulse: '+str(np.argmax(ys_basesub[0][checkp]> 2000))) ## we can follow the index as it changes with extension/cut\n",
    "# #print('extended range index for checkpulse: '+str(np.argmax(ys_ext[0][checkp]> 2000)))\n",
    "# print('cut array index for checkpulse: '+str(np.argmax((ys_cut[0][checkp]*HeNorms[checkp])> 2000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ys_ext ## might help with memory issues\n",
    "del ys_basesub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print('aligning and cutting time: ' + str(end-start))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "begin SF organization ##<br>\n",
    "In[11]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_SF(SFsort_info): ## sometimes pulse 0 has the state switch. In that case, need to account by if clauses below\n",
    "    counter = 0\n",
    "    seq = 0\n",
    "    seq_arr = ([[],[],[]])\n",
    "    smallerseq = []\n",
    "    smallerstateis = []\n",
    "    for i in range(len(SFsort_info[1])-(np.mod((len(SFsort_info[1])), 8))):  ##111 mod 8 = 7, so essentially 111-7 = 104\n",
    "        counter = counter+1\n",
    "        if counter < 8:\n",
    "            if (SF_Sort_arr[1][i]) == 0: ## catches state switches at pulse 0\n",
    "                smallerstateis.append([(SFsort_info[1][i])+5,(SFsort_info[1][i+1])])\n",
    "                smallerseq.append(SFsort_info[0][i+1])\n",
    "                seq = seq+1\n",
    "                continue\n",
    "            smallerstateis.append([(SFsort_info[1][i])+5,(SFsort_info[1][i+1])])\n",
    "            smallerseq.append(SFsort_info[0][i+1])\n",
    "        elif counter == 8:\n",
    "            if ((SF_Sort_arr[1][i])+45) >= 5000: ## breaks for state switches at pulse 0\n",
    "                print(((SF_Sort_arr[1][i])+5))\n",
    "                seq = seq+1\n",
    "                seq_arr[0].append(seq)\n",
    "                seq_arr[1].append(smallerseq)   \n",
    "                seq_arr[2].append(smallerstateis)\n",
    "                seq_arr[0] = [x-1 for x in seq_arr[0]] ## reset so sequences are 1-14 instead of 2-15\n",
    "                break\n",
    "            seq = seq+1 ## otherwise continue regular sorting\n",
    "            smallerstateis.append([(SFsort_info[1][i])+5,(SFsort_info[1][i+1])])\n",
    "            smallerseq.append(SFsort_info[0][i+1])\n",
    "            seq_arr[0].append(seq)\n",
    "            seq_arr[1].append(smallerseq)   \n",
    "            seq_arr[2].append(smallerstateis)\n",
    "            smallerseq = []\n",
    "            smallerstateis = []\n",
    "            counter  = 0\n",
    "    return seq_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_leftover(SFsort_info, seq_arr): ## in case we want to use the other 6 states left over\n",
    "    left = [[seq_arr[0][-1]+1],[],[]]\n",
    "    counter = 0\n",
    "    for i in range((len(SFsort_info[1])-(np.mod((len(SFsort_info[1])), 8))), len(SFsort_info[1])-1):\n",
    "        counter = counter+1\n",
    "        if counter < 8:\n",
    "            left[1].append(SFsort_info[0][i+1])\n",
    "            left[2].append([(SFsort_info[1][i])+5,(SFsort_info[1][i+1])])\n",
    "    return left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################### SF organization commented out for pol ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('skipped SF organization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[12]<br>\n",
    "try:<br>\n",
    "    sequence = organize_SF(SF_Sort_arr)<br>\n",
    "    if len(sequence[0]) == 14: ## catches state switches at pulse 0, leftovers are at the end of the regular sequence<br>\n",
    "        leftovers = [[sequence[0][-1]],[sequence[1][-1]],[sequence[2][-1]]]<br>\n",
    "        for i in range(len(sequence)):<br>\n",
    "            sequence[i].pop(-1) ## deletes the leftovers sequence for state switches at pulse 0<br>\n",
    "    else:<br>\n",
    "        leftovers = find_leftover(SF_Sort_arr, sequence) ## otherwise can use normal function<br>\n",
    "except Exception as e:<br>\n",
    "    logger.error('run '+run_num + ' failed during sequencing')<br>\n",
    "    logger.exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(str(len(sequence[0]))+' sequences with sequence order: '+str(sequence[1][0]))<br>\n",
    "print(leftovers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " add up pulses for their respective state, in each 8 step sequence ##<br>\n",
    " turning into a by-channel function 06.13.24 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = time.time()<br>\n",
    "sequence = np.asarray(sequence, dtype = object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ON_sums = np.zeros((len(ys_cut), len(sequence[0]), len(ys_cut[0][0])), dtype=np.float64) ## 13 channels, 13 sequences, added pulses for ON<br>\n",
    "OFF_sums = np.zeros((len(ys_cut), len(sequence[0]), len(ys_cut[0][0])), dtype=np.float64) ## 13 channels, 13 sequences, added pulses for OFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@njit<br>\n",
    "def add_pulse(ys, SFarr):<br>\n",
    "    temp_ON = np.zeros((len(SFarr[0]), len(ys[0])), dtype=np.float64)<br>\n",
    "    temp_OFF = np.zeros((len(SFarr[0]), len(ys[0])), dtype=np.float64)<br>\n",
    "    for seq in range(0, len(SFarr[0])): ## for every sequence<br>\n",
    "    #         print('seq:' +str(SFarr[0][seq]))<br>\n",
    "#         print('seq:' +str(seq))<br>\n",
    "        for state in range(0, len(SFarr[1][0])): ## for every state in the sequence<br>\n",
    "    #         print(\"states loop \" + str(range(0, len(SFarr[1][0]))[0]) + ' - ' +  str(range(0, len(SFarr[1][0]))[-1]))<br>\n",
    "            s = SFarr[1][seq][state] ## try this to condense code. Basically, the state currently at<br>\n",
    "            if s==0 or s==3 or s==5 or s==6: ## these are ON states<br>\n",
    "#                 print('ON \"s\" state '+str(s))<br>\n",
    "#                 print('\"state\" ' +str(state) + ' from ' + str(range((SFarr[2][seq][state][0]),(SFarr[2][seq][state][1]))))<br>\n",
    "#                 print('sums from '+str(range((SFarr[2][seq][state][0]),(SFarr[2][seq][state][1]))[0]) +<br>\n",
    "#                 ' - ' +str(range((SFarr[2][seq][state][0]),(SFarr[2][seq][state][1]))[-1]) + '\\n')<br>\n",
    "                for p in range((SFarr[2][seq][state][0]),(SFarr[2][seq][state][1])): ##From 20-60 for example. SFarr[2] is the array of start to end pulses to sum<br>\n",
    "                    temp_ON[seq] = np.add(temp_ON[seq],ys[p]) ## start with zeros, add to each iteratively<br>\n",
    "            if s==1 or s==2 or s==4 or s==7: ## these are OFF states<br>\n",
    "#                 print('OFF \"s\" state '+str(s))<br>\n",
    "#                 print('\"state\" ' +str(state) + ' from ' + str(range((SFarr[2][seq][state][0]),(SFarr[2][seq][state][1]))))<br>\n",
    "#                 print('sums from '+str(range((SFarr[2][seq][state][0]),(SFarr[2][seq][state][1]))[0]) +<br>\n",
    "#                 ' - ' +str(range((SFarr[2][seq][state][0]),(SFarr[2][seq][state][1]))[-1]) + '\\n')<br>\n",
    "                for p in range((SFarr[2][seq][state][0]),(SFarr[2][seq][state][1])):<br>\n",
    "                    temp_OFF[seq] = np.add(temp_OFF[seq],ys[p])<br>\n",
    "    return temp_ON, temp_OFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####################################################################################<br>\n",
    "In[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "## new pulse add for polarization attempt ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ys_basesub = np.zeros((len(ys_arr), numRuns,numSamples[0]), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_pulses = np.zeros((len(ys_cut), len(ys_cut[0][0])), dtype=np.float64)  ## channels, 8992 data points each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pulse_noSF(ys):\n",
    "    pulses_sum  = np.zeros((len(ys[0])), dtype=np.float64)\n",
    "    for p in range(0, len(ys)):\n",
    "        # print('pulse ',p)\n",
    "        pulses_sum = np.add(pulses_sum,ys[p])\n",
    "    return pulses_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ys_cut)):\n",
    "    # print('#################### channel: ' + str(i) + ' ##########################')\n",
    "    added_pulses[i] = add_pulse_noSF(ys_cut[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print('summing pulses time: ' + str(end-start))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing ####<br>\n",
    "for i in range(0, len(added_pulses)):<br>\n",
    "    plt.plot(xs_cut[0], added_pulses[i], label = 'chan ' + str(chan_enab[i]))<br>\n",
    "plt.legend()<br>\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################### Don't need BG/Voigt fitting here (?), and no asymm ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[15]:<br>\n",
    "bg fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from scipy import odr<br>\n",
    "from scipy.special import voigt_profile<br>\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bg_region1_beg = 4180<br>\n",
    "bg_region1_end = 5450<br>\n",
    "bg_region2_beg = 6250<br>\n",
    "bg_region2_end = 8992<br>\n",
    "bg_reg1 = [bg_region1_beg,bg_region1_end]<br>\n",
    "bg_reg2 = [bg_region2_beg,bg_region2_end]<br>\n",
    "fullrange = bg_reg2[1]-bg_reg1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In[18]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change to 2nd order poly in cleaned up version<br>\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def bg_fitsubtract(bef_res_reg, aft_res_reg, ys): ## before/after resonance region [start:end] respectively, ys[ch] to fit<br>\n",
    "    binstot = aft_res_reg[1]-bef_res_reg[0]  ## total number of bins in whole region<br>\n",
    "    x1 = np.arange(bef_res_reg[0], bef_res_reg[1],1)<br>\n",
    "    x2 = np.arange(aft_res_reg[0], aft_res_reg[1],1)<br>\n",
    "    x = np.append(x1,x2)<br>\n",
    "    fullx = np.arange(bef_res_reg[0], aft_res_reg[1],1) ## an array of every x bin in entire region<br>\n",
    "    ys_bgsub = []<br>\n",
    "    for seq in range(0, len(ys)): ## number of sequences, usually 13<br>\n",
    "        fitdata1 = ys[seq][bef_res_reg[0]: bef_res_reg[1]]<br>\n",
    "        fitdata2 = ys[seq][aft_res_reg[0]: aft_res_reg[1]]<br>\n",
    "        datasplice = np.append(fitdata1, fitdata2)<br>\n",
    "        y = datasplice<br>\n",
    "        data = odr.Data(x, y)<br>\n",
    "        poly_model2 = odr.polynomial(2)  # using third order polynomial model<br>\n",
    "        odr_obj = odr.ODR(data, poly_model2)<br>\n",
    "        output = odr_obj.run()  # running ODR fitting<br>\n",
    "        poly2 = np.poly1d(output.beta[::-1])<br>\n",
    "        poly_y2 = poly2(x)<br>\n",
    "        fullpoly_y2 = poly2(fullx)<br>\n",
    "        bgsubtracted = ys[seq][bef_res_reg[0]:aft_res_reg[1]] - fullpoly_y2  ## subtracts RealData-BackgroundFit<br>\n",
    "        ys_bgsub.append(bgsubtracted)<br>\n",
    "    return ys_bgsub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## background subtraction currently only for NaI detectors<br>\n",
    "num_fittingchs = len(ON_sums)-1  ## removes the Li detector, assumes it is there<br>\n",
    "ON_bgsub = np.zeros((num_fittingchs,len(ON_sums[0]),fullrange), dtype = np.float64) ## channels, sequences, range of bg_ subtraction<br>\n",
    "OFF_bgsub = np.zeros((num_fittingchs,len(ON_sums[0]),fullrange), dtype = np.float64) ## channels, sequences, range of bg_ subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(0, len(ON_sums)-1):<br>\n",
    "    if chan_enab[i] == 24:<br>\n",
    "        emessage = ('bg_ fit does not work for 6Li yet')<br>\n",
    "        logger.error('run '+run_num + emessage)<br>\n",
    "        raise Exception(emessage)<br>\n",
    "    ON_bgsub[i]  = bg_fitsubtract(bg_reg1,bg_reg2, ON_sums[i])<br>\n",
    "    OFF_bgsub[i] = bg_fitsubtract(bg_reg1,bg_reg2, OFF_sums[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end = time.time()<br>\n",
    "# print('bg_ fitting time: ' + str(end-start))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In[19]:<br>\n",
    "# now voigt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def voigt2(x, sig, gam, xshift, amp):<br>\n",
    "    fit = voigt_profile(x-xshift, sig, gam)*amp<br>\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In[21]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# res_region_beg = 5450<br>\n",
    "# res_region_end = 6250<br>\n",
    "res_region_beg = bg_region1_end  ##currently res region is not selectable... inside of background region<br>\n",
    "res_region_end = bg_region2_beg<br>\n",
    "res_reg = [res_region_beg,res_region_end]<br>\n",
    "if res_reg[0] < bg_reg1[1] or res_reg[1]>bg_reg2[0]:<br>\n",
    "    emessage = ('Declared Background region and Resonance region have overlapping fitting regions')<br>\n",
    "    logger.error('run '+run_num + emessage)<br>\n",
    "    raise Exception(emessage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xdata = xs_cut[0][res_reg[0]:res_reg[1]]*1e-6  ## just change all xs to ms and one array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In[22]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resonance region is assumed to be in between 2 background regions. Could maybe change this. 11.22.24<br>\n",
    "# start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# def voigt_fitting(bef_res_reg, aft_res_reg,xs,ys):\n",
    "#     fit_curves = []\n",
    "#     parameters = []\n",
    "#     for seq in range(0, len(ys)): ## number of sequences, usually 13\n",
    "#         ydata = ys[seq][bef_res_reg[1]-bef_res_reg[0]:aft_res_reg[0]-bef_res_reg[0]] \n",
    "#         popt, pcov = curve_fit(voigt2, xs, ydata, bounds = ([0,0,0,-np.inf], [np.inf,np.inf,np.inf, np.inf]))\n",
    "#         fitted_curve = voigt2(xs, popt[0],popt[1],popt[2],popt[3],) ## sigma, gamma, xshift (res. center), amp. related thing\n",
    "#         fit_curves.append(fitted_curve)\n",
    "#         fit_params = popt\n",
    "#         fit_errs = np.diagonal(pcov)\n",
    "#         parameters.append([fit_params,fit_errs])\n",
    "#     return fit_curves, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res_size = res_reg[1]-res_reg[0] <br>\n",
    "ON_vfit = np.zeros((len(ON_bgsub),len(ON_bgsub[0]),res_size), dtype = np.float64) ## channels, sequences, range of V_ subtraction<br>\n",
    "OFF_vfit = np.zeros((len(ON_bgsub),len(ON_bgsub[0]),res_size), dtype = np.float64) ## channels, sequences, range of V_ subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ON_vfit_params = np.zeros((len(ON_bgsub),len(ON_bgsub[0]),2,4), dtype = np.float64) ## channels, sequences,[params, param_errs], [sigma, gamma, shift, amp thing]<br>\n",
    "OFF_vfit_params = np.zeros((len(ON_bgsub),len(ON_bgsub[0]),2,4), dtype = np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(0, len(ON_sums)-1): <br>\n",
    "    if chan_enab[i] == 24:<br>\n",
    "        emessage = ('bg_ fit does not work for 6Li yet')<br>\n",
    "        logger.error('run '+run_num + emessage)<br>\n",
    "        raise Exception(emessage)<br>\n",
    "    try:<br>\n",
    "        ON_vfit[i], ON_vfit_params[i]  = voigt_fitting(bg_reg1,bg_reg2,xdata, ON_bgsub[i])<br>\n",
    "        OFF_vfit[i], OFF_vfit_params[i] = voigt_fitting(bg_reg1,bg_reg2,xdata, OFF_bgsub[i])<br>\n",
    "    except Exception as e:<br>\n",
    "        logger.error('run '+run_num + ' failed during Voigt fitting')<br>\n",
    "        logger.exception(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end = time.time()<br>\n",
    "print('bg_ fitting time: ' + str(end-start)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In[23]:<br>\n",
    "sum up all pulses, and then calculate asymmetry using \"amplitude\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = time.time()<br>\n",
    "all_ONOFF = np.zeros((len(ON_sums),2, len(ON_sums[0][0])), dtype=np.float64) ## all summed ON[0] and OFF[1] pulses for each channel, not each sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def all_onoff(ON_arr, OFF_arr):<br>\n",
    "    tempallON = np.zeros((len(ON_arr[0])), dtype=np.float64)<br>\n",
    "    tempallOFF = np.zeros((len(ON_arr[0])), dtype=np.float64)<br>\n",
    "    for seq in range(0, len(ON_arr)): ## number of sequences<br>\n",
    "        tempallON = np.add(ON_arr[seq],tempallON)<br>\n",
    "        tempallOFF = np.add(OFF_arr[seq],tempallOFF)<br>\n",
    "    return tempallON, tempallOFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(len(ON_sums)):<br>\n",
    "    all_ONOFF[i][0], all_ONOFF[i][1] = all_onoff(ON_sums[i], OFF_sums[i]) ## channel, ON[0] or OFF[1], summed wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In[25]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_Vfitcurve_ON = np.zeros((len(ON_vfit), len(ON_vfit[0][0])), dtype=np.float64) ## all summed ON or OFF pulses for each channel, not each sequence<br>\n",
    "all_Vfitcurve_OFF = np.zeros((len(ON_vfit), len(ON_vfit[0][0])), dtype=np.float64) ## all summed ON or OFF pulses for each channel, not each sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def all_vfits(vfit_arrs):<br>\n",
    "    tempall_vfits = np.zeros((len(vfit_arrs[0])), dtype=np.float64)<br>\n",
    "    for seq in range(0, len(vfit_arrs)): ## number of sequences<br>\n",
    "        tempall_vfits = np.add(vfit_arrs[seq],tempall_vfits)<br>\n",
    "    return tempall_vfits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(len(ON_vfit)):<br>\n",
    "    all_Vfitcurve_ON[i]  = all_vfits(ON_vfit[i]) ## all added vfits for each channel<br>\n",
    "    all_Vfitcurve_OFF[i] = all_vfits(OFF_vfit[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In[26]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add and avg all sigma/gamma/xloc for ON and OFF per channel. Should not be dpendent on spin state.<br>\n",
    "sig_ch = np.zeros((len(ON_vfit),2), dtype=np.float64) ## 1 sigma +/- error for each channel<br>\n",
    "gam_ch = np.zeros((len(ON_vfit),2), dtype=np.float64) ## 1 gamma +/- error for each channel<br>\n",
    "xloc_ch = np.zeros((len(ON_vfit),2), dtype=np.float64) ## 1 xlocation +/- error for each channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def sum_param(ON_params_arr, OFF_params_arr, key):  ## key choose between sigma gamma or xloc<br>\n",
    "    if key != 'sigma' and key != 'gamma' and key != 'xloc':<br>\n",
    "        emessage = ('not a valid Voigt fit parameter to sum')<br>\n",
    "        logger.error('run '+run_num + emessage)<br>\n",
    "        raise Exception(emessage)<br>\n",
    "    if key == 'sigma':<br>\n",
    "        param = 0<br>\n",
    "    if key == 'gamma':<br>\n",
    "        param = 1<br>\n",
    "    if key == 'xloc':<br>\n",
    "        param = 2<br>\n",
    "    tempsum = 0<br>\n",
    "    temperr  = []<br>\n",
    "    for seq in range(0, len(ON_params_arr)): ## number of sequences<br>\n",
    "        seqsum = (ON_params_arr[seq][0][param]+OFF_params_arr[seq][0][param])/2  # \"normalize\" with /2 but not for #sequences<br>\n",
    "        ## [0] above corresponds to the real values of the paramters, as opposed to their errors<br>\n",
    "        ON_err = ON_params_arr[seq][1][param]<br>\n",
    "        OFF_err = OFF_params_arr[seq][1][param]<br>\n",
    "        ## above [1] is used which corresponds to the error in \"parameter\"<br>\n",
    "        ON_deriv  =  1  ## left over from asym error to keep error prop consistent<br>\n",
    "        OFF_deriv =  1<br>\n",
    "        ## use err prop to get below<br>\n",
    "        seq_err = np.sqrt((ON_deriv**2)*(ON_err**2)+(OFF_deriv**2)*(OFF_err**2))/2<br>\n",
    "        temperr.append(seq_err) ## collect all errors for each sequence<br>\n",
    "        tempsum = np.add(seqsum,tempsum)<br>\n",
    "    ## add error or each sequence in quad.<br>\n",
    "    toterr = np.sqrt(sum([i**2 for i in temperr]))  ## use list comprehension for sum of squares<br>\n",
    "    out = [tempsum,toterr]<br>\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(len(ON_vfit)):<br>\n",
    "    sig_ch[i] = sum_param(ON_vfit_params[i], OFF_vfit_params[i], key='sigma')<br>\n",
    "    gam_ch[i] = sum_param(ON_vfit_params[i], OFF_vfit_params[i], key='gamma') <br>\n",
    "    xloc_ch[i] = sum_param(ON_vfit_params[i], OFF_vfit_params[i], key='xloc') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In[27]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new asym using \"amplitude\" parameter, with error prop<br>\n",
    "asym_ch_err = np.zeros((len(ON_vfit),2), dtype=np.float64) ## 1 Asym for each channel, not for each sequence (can change?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def asym3_err(ON_params_arr, OFF_params_arr):<br>\n",
    "    tempasym = 0<br>\n",
    "    temperr  = []<br>\n",
    "    for seq in range(0, len(ON_params_arr)): ## number of sequences<br>\n",
    "        A_plus = ON_params_arr[seq][0][-1] ## just makes things cleaner<br>\n",
    "        A_min  = OFF_params_arr[seq][0][-1]<br>\n",
    "        ## [0] corresponds to the real values of the paramters, as opposed to their errors. [-1] is the \"amplitude\"<br>\n",
    "        seqasym = ((A_plus-A_min) / (A_plus+A_min))<br>\n",
    "        ON_err = ON_params_arr[seq][1][-1]<br>\n",
    "        OFF_err = OFF_params_arr[seq][1][-1]<br>\n",
    "        ## above [1] is used which corresponds to the error in \"amplitude\"<br>\n",
    "        ON_deriv  =  2*A_min/ ((A_plus+A_min)**2)<br>\n",
    "        OFF_deriv = -2*A_plus/((A_plus+A_min)**2)<br>\n",
    "        ## use err prop to get below<br>\n",
    "        seq_err = np.sqrt((ON_deriv**2)*(ON_err**2)+(OFF_deriv**2)*(OFF_err**2))<br>\n",
    "#         seq_err = (1/(A_plus+A_min**2))*np.sqrt(4*(A_min**2)(ON_err)**2+4*(A_plus**2)(OFF_err)**2) ## alternate form<br>\n",
    "        temperr.append(seq_err) ## collect all errors for each sequence<br>\n",
    "        tempasym = np.add(seqasym,tempasym)<br>\n",
    "#         print(temperr)<br>\n",
    "    ## add error or each sequence asym in quad.<br>\n",
    "    toterr = np.sqrt(sum([i**2 for i in temperr]))  ## use list comprehension for sum of squares<br>\n",
    "    out = [tempasym,toterr]<br>\n",
    "    return out  ## don't normalize asymON_vfit_params this time..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(len(ON_vfit)):<br>\n",
    "    asym_ch_err[i] = asym3_err(ON_vfit_params[i], OFF_vfit_params[i]) ## error is really high..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# end = time.time()\n",
    "# print('calc asym, add pulses/fit params: ' + str(end-start))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[28]:<br>\n",
    "save summed pulses to be analyzed for polarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " new version adds vfit curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(polSavename+'.h5', 'w') as hdf5_file:\n",
    "    hdf5_file.create_dataset('xs ', data=xs_cut[0]) ## all xs are the same, even though they are per channel...\n",
    "    # hdf5_file.attrs['sequences'] = len(sequence[0])\n",
    "    for i in range(0,len(ys_cut)):\n",
    "        Ch_grp = hdf5_file.create_group('ch_'+str(np.char.zfill(str(chan_enab[i]), 2)))\n",
    "        added_pulses_subgrp = Ch_grp.create_group('added_pulses')\n",
    "        # added_vfits_subgrp = Ch_grp.create_group('added_vfits')\n",
    "        # added_pulses_subgrp.attrs['rownames'] = ['ON_Row0', 'OFF_Row1']\n",
    "        added_pulses_subgrp.create_dataset('ch_'+str(np.char.zfill(str(chan_enab[i]), 2)), data=added_pulses[i])\n",
    "        # if chan_enab[i] == 24:\n",
    "        #     pass\n",
    "        # else:\n",
    "        #     added_vfits_subgrp.attrs['resonance_region'] = res_reg\n",
    "        #     added_vfits_subgrp.attrs['note'] = 'added all Voigt fits for each ON or OFF state, per ch.'\n",
    "        #     added_vfits_subgrp.create_dataset('Vfit_curve_ON',data=all_Vfitcurve_ON[i])\n",
    "        #     added_vfits_subgrp.create_dataset('Vfit_curve_OFF',data=all_Vfitcurve_OFF[i])\n",
    "        \n",
    "# end = time.time()\n",
    "# print('saving hdf5: ' + str(end-start))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[30]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = time.time()<br>\n",
    "AsymSavename_raw = AsymSavename<br>\n",
    "with h5py.File(AsymSavename+'.h5', 'w') as hdf5_file:<br>\n",
    "    hdf5_file.create_dataset('xs ', data=xs_cut[0]) ## all xs are the same, even though they are per channel...<br>\n",
    "    hdf5_file.attrs['sequences'] = len(sequence[0])<br>\n",
    "    for i in range(0,len(asym_ch_err)): ## change this to length of asymms!<br>\n",
    "        Ch_grp = hdf5_file.create_group('ch_'+str(np.char.zfill(str(chan_enab[i]), 2)))<br>\n",
    "        Ch_grp.attrs['quick asym'] = asym_ch_err[i]<br>\n",
    "        Ch_grp.attrs['avg_xloc']  = xloc_ch[i]<br>\n",
    "        Ch_grp.attrs['avg_sigma'] = sig_ch[i]<br>\n",
    "        Ch_grp.attrs['avg_gamma'] = gam_ch[i]<br>\n",
    "        ON_subgrp = Ch_grp.create_group('ON')<br>\n",
    "        OFF_subgrp = Ch_grp.create_group('OFF')<br>\n",
    "        ON_subgrp.attrs['for_each_sequence'] = ['parameter (sigma, gamma, x_loc, \"amplitude\")', 'and its error']<br>\n",
    "        OFF_subgrp.attrs['for_each_sequence'] = ['parameter (sigma, gamma, x_loc, \"amplitude\")', 'and its error']<br>\n",
    "        ON_subgrp.create_dataset('parameters',data=ON_vfit_params[i])<br>\n",
    "        OFF_subgrp.create_dataset('parameters',data=OFF_vfit_params[i])<br>\n",
    "        ON_subgrp.create_dataset('Vfit_curves',data=ON_vfit[i])<br>\n",
    "        OFF_subgrp.create_dataset('Vfit_curves',data=OFF_vfit[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "end = time.time()\n",
    "print('saving hdf5: ' + str(end-start))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In[31]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullend = time.time()\n",
    "print('full processing time: ' + str(fullend-fullstart))  \n",
    "print('finished ' + str(datetime.now())) \n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end of data processing ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
