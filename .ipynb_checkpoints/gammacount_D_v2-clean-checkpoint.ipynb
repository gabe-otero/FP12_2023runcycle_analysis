{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "#import uproot\n",
    "import pandas as pd\n",
    "import statistics as st\n",
    "import os\n",
    "from numba import njit\n",
    "import time\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\n",
    "import warnings\n",
    "from loguru import logger\n",
    "from datetime import datetime\n",
    "from numba.typed import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data: debug_sample/runs12034-12363//run12034\n",
      "Channel is [ 0  1  2  3  4  5  6  7  8  9 10 11 24]\n"
     ]
    }
   ],
   "source": [
    "# warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "# warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n",
    "# warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# for arg in sys.argv:\n",
    "#     run_num=str(arg).zfill(5)\n",
    "#     # print(run_num)\n",
    "\n",
    "# chan_enab = int(sys.argv[-1])\n",
    "# run_start=str(sys.argv[1]).zfill(5)\n",
    "# run_end=str(sys.argv[2]).zfill(5)\n",
    "# run_num=str(sys.argv[3]).zfill(5)\n",
    "\n",
    "run_num = '12034'\n",
    "runs_folder = 'runs12034-12363/'\n",
    "os.chdir('F:/LANL/')\n",
    "datadir = 'sample_data/'\n",
    "uniquefolder = 'debug_sample/'+runs_folder\n",
    "SFNormFile = 'SF_Norm_files/'+runs_folder+run_num\n",
    "statefileloc = 'F:\\LANL\\SF_Norm_files\\TR_R_expected_avgs_stds_afterclip.csv'\n",
    "\n",
    "# print(os.getcwd())\n",
    "\n",
    "# os.chdir('F:/LANL/')\n",
    "# datadir = 'D:/LANSCE_FP12_2023/data/' ## add directory of hard drive\n",
    "# uniquefolder = \"runs\" + str(run_start) + \"-\" + str(run_end) +\"/\"\n",
    "# SFNormFile = 'SF_Norm_files/'+uniquefolder+run_num\n",
    "\n",
    "print('processing data: ' + uniquefolder + '/run' + run_num)\n",
    "\n",
    "# statefileloc = 'F:\\LANL\\SF_Norm_files\\TR_R_expected_avgs_stds_afterclip.csv'\n",
    "# processedpulsefolder = '/processed_data/'+uniquefolder+'pulses_added_D/'\n",
    "# processedasymfolder = '/processed_data/'+uniquefolder+'asym_D/'\n",
    "# AddedPulseSavename = processedpulsefolder+run_num+'_pulsesadded_D'\n",
    "# AsymSavename = processedasymfolder+run_num+'_asym_D'\n",
    "# ONSavename = processedasymfolder+run_num+'_ON_D'\n",
    "# OFFSavename = processedasymfolder+run_num+'_OFF_D'\n",
    "# logger.add(\"F:/LANL/processed_data/\" + uniquefolder + '0_ErrorLog_'+run_start+'_'+run_end+'_D.txt', delay = False)\n",
    "# print('saving processed data to ' + AsymSavename)\n",
    "\n",
    "## cannot handle all 24 detectors at once, memory issue... can look into np.empty and deleting variables if needed<br>\n",
    "# chan_enab = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]) ## all\n",
    "chan_enab = np.array([0,1,2,3,4,5,6,7,8,9,10,11,24]) ## downstream\n",
    "# chan_enab = np.array([12,13,14,15,16,17,18,19,20,21,22,23,24]) ## upstream\n",
    "\n",
    "# if not os.path.exists(os.getcwd()+processedpulsefolder) or not os.path.exists(os.getcwd()+processedasymfolder):\n",
    "#     # Create the directory\n",
    "#     os.makedirs(os.getcwd()+processedpulsefolder)\n",
    "#     os.makedirs(os.getcwd()+processedasymfolder)\n",
    "#     print(\"Directory created successfully\")\n",
    "# else:\n",
    "#     pass\n",
    "\n",
    "start = time.time()\n",
    "fullstart = time.time()\n",
    "\n",
    "# read_data = np.array([])\n",
    "# fileLength = np.array([])\n",
    "read_data = []\n",
    "fileLength = []\n",
    "\n",
    "def open_file():\n",
    "    for el in chan_enab:\n",
    "        # f = open(datadir + folder + 'run' + run_num + \"_ch\" + str(el) + \".bin\", 'rb')\n",
    "        f = open(datadir+uniquefolder + 'run' + str(run_num) + \"_ch\" +str(el) + \".bin\", 'rb')\n",
    "        read_data.append(np.fromfile(file=f, dtype=np.uint16))\n",
    "        f.close()\n",
    "        fileLength.append(len(read_data[-1]))\n",
    "    return read_data, fileLength\n",
    "\n",
    "open_file()\n",
    "\n",
    "fileLength = np.asarray(fileLength)\n",
    "read_data = np.asarray(read_data) ## in detector's case, all are the same size samples, so can do read_data as np array\n",
    "\n",
    "if chan_enab[-1] != 24:\n",
    "    emessage = ('last channel is not 6Li detector')\n",
    "    logger.error(run_num + emessage)\n",
    "    raise Exception(emessage)\n",
    "\n",
    "# print('saving processed data to ' + AsymSavename)\n",
    "print(\"Channel is \" + str(chan_enab))\n",
    "end = time.time()\n",
    "# print('file open time: ' + str(end-start))      \n",
    "# print(read_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the big header for each channel in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target is La\n"
     ]
    }
   ],
   "source": [
    "BoardID = []\n",
    "recordLength = []\n",
    "numSamples = []\n",
    "eventCounter = []\n",
    "decFactor = []\n",
    "chanDec = []\n",
    "postTrig = []\n",
    "groupStart = []\n",
    "groupEnd = []\n",
    "timestamp= []\n",
    "sizeFirstEvent = []\n",
    "TTT = []\n",
    "\n",
    "targetDict = {0: \"La\", 1: \"Tb2O3\", 2: \"Yb2O3\", 3: \"Sm2O3\", 4: \"Er2O3\", 5: \"Ho2O3\", 6: \"other\"}\n",
    "foilDict = {0: \"TBD\", 1: \"TBD\", 2: \"TBD\", 3: \"TBD\", 4: \"TBD\", 5: \"TBD\", 6: \"other\"}\n",
    "\n",
    "target=(read_data[0][5]&0x00F0)>>4\n",
    "foil=read_data[0][5]&0x000F\n",
    "targetFlag = read_data[0][5]>>8&1\n",
    "foilFlag = read_data[0][5]>>9&1\n",
    "spinFiltFlag = read_data[0][5]>>10&1\n",
    "spinFlipFlag = read_data[0][5]>>11&1\n",
    "shutterFlag = read_data[0][5]>>12&1\n",
    "facilityTrigFlag = read_data[0][5]>>13&1\n",
    "\n",
    "if targetFlag:\n",
    "    target=targetDict[(read_data[0][5]&0x00F0)>>4]\n",
    "else:\n",
    "    target = \"empty\"\n",
    "if foilFlag:\n",
    "    foil=foilDict[read_data[0][5]&0x000F]\n",
    "else:\n",
    "    foil = \"empty\"\n",
    "for i in range(0,len(chan_enab)):\n",
    "    BoardID.append(read_data[i][9]>>8)\n",
    "    recordLength.append(((read_data[i][9]&0x00FF)<<16)+read_data[i][8])\n",
    "    numSamples.append(((read_data[i][11]&0x00FF)<<16)+read_data[i][10])\n",
    "    eventCounter.append(read_data[i][6]+(read_data[i][7]<<16))\n",
    "    BoardID.append(read_data[i][9]>>8)  \n",
    "    decFactor.append(read_data[i][11]>>8)\n",
    "    chanDec.append(read_data[i][13]>>8)\n",
    "    postTrig.append(read_data[i][15]>>8)\n",
    "    groupStart.append(((read_data[i][13]&0x00FF)<<16)+read_data[i][12])\n",
    "    groupEnd.append(((read_data[i][15]&0x00FF)<<16)+read_data[i][14])\n",
    "    timestamp.append(read_data[i][16]+(read_data[i][17]<<16)+(read_data[i][18]<<32)+(read_data[i][19]<<40))  \n",
    "    sizeFirstEvent.append(read_data[i][0]+(read_data[i][1]<<16))\n",
    "    TTT.append(read_data[i][2]+(read_data[i][3]<<16)+(read_data[i][4]<<32))\n",
    "    \n",
    "#     print(\"For channel \" + str(chan_enab[i]) + \", BoardID is \" + str(BoardID[i])\n",
    "#           + \"; record length is \" + str(recordLength[i]) + \"; num Samples is \" \n",
    "#           + str(numSamples[i]) + \"; event counter is \" + str(eventCounter[i]) + \"; dec factor is \" + str(decFactor[i]) + \"; chan dec is \" \n",
    "#           + str(chanDec[i]) + \"; postTrig is \" + str(postTrig[i]) + \"; group start is \" + str(groupStart[i]) + \"; group end is \" + str(groupEnd[i])\n",
    "#           + \"; epoch time is \" + str(timestamp[i]) +  \"; first event size is \" + str(sizeFirstEvent[i]) + \"; and ETTT is \" + str(TTT[i]) + \"\\n\")\n",
    "\n",
    "BoardID = np.asarray(BoardID) \n",
    "recordLength = np.asarray(recordLength)\n",
    "numSamples = np.asarray(numSamples)\n",
    "eventCounter = np.asarray(eventCounter)\n",
    "decFactor = np.asarray(decFactor)\n",
    "chanDec = np.asarray(chanDec)\n",
    "postTrig = np.asarray(postTrig)\n",
    "groupStart = np.asarray(groupStart)\n",
    "groupEnd = np.asarray(groupEnd)\n",
    "timestamp = np.asarray(timestamp)\n",
    "sizeFirstEvent = np.asarray(sizeFirstEvent)\n",
    "TTT = np.asarray(TTT)\n",
    "print(\"Target is \" + target)\n",
    "# print(\"Foil is \" + foil)\n",
    "# print(\"Shutter is open: \" + str(bool(shutterFlag)))\n",
    "# print(\"Facility t0 is on: \" + str(bool(facilityTrigFlag)))\n",
    "# print(\"Spin flipper is on: \" + str(bool(spinFlipFlag)))\n",
    "# print(\"Spin filter is on: \" + str(bool(spinFiltFlag)))\n",
    "# print(\"Target is present: \" + str(bool(targetFlag)))\n",
    "# print(\"Foil is present: \" + str(bool(foilFlag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the time axis for each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preTime = []\n",
    "startTime = []\n",
    "endTime = []\n",
    "resolution = []\n",
    "xs = [] \n",
    "\n",
    "for i in range(0,len(chan_enab)):\n",
    "    preTime.append((100-postTrig[i])*recordLength[i]/100)\n",
    "    startTime.append((-1*preTime[i]*16*decFactor[i] + groupStart[i]*16*decFactor[i]))\n",
    "    endTime.append((-1*preTime[i]*16*decFactor[i] + groupEnd[i]*16*decFactor[i]))\n",
    "    resolution.append(16*chanDec[i]*decFactor[i])\n",
    "#     print(\"Pretime for channel\", chan_enab[i],\"is \" + str(preTime[i]) + \"; start time is \" + str(startTime[i]) + \"; end time is \" + str(endTime[i]) \n",
    "#           + \"; resolution is \" + str(resolution[i]) + \"ns\")\n",
    "    xs.append(np.arange(startTime[i],(numSamples[i])*resolution[i]+startTime[i], resolution[i]))\n",
    "\n",
    "# np.asarray(preTime) \n",
    "# np.asarray(startTime) \n",
    "# np.asarray(endTime) \n",
    "# np.asarray(resolution)\n",
    "xs = np.asarray(xs) ## can convert xs to np array here because all detectors same numsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataread from binary time: 3.9658455848693848\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "@njit\n",
    "def dataread(data, channels, fileLen, numSamps):\n",
    "    numRuns = int((fileLen[0]-20-numSamps[0])/(numSamps[0]+6)+1)\n",
    "    ys_arr = np.zeros((len(channels), numRuns,numSamps[0]), dtype=np.uint16)\n",
    "    ETTT_arr = np.zeros((len(channels), numRuns), dtype=np.intc)\n",
    "    eventcount_arr = np.zeros((len(channels), numRuns), dtype=np.intc)\n",
    "    for i in range(0,len(channels)):\n",
    "        eventCount = 0\n",
    "        byteCounter = 0\n",
    "            #byte counter is really 2bytecounter, lol\n",
    "        while byteCounter < fileLen[i]:\n",
    "            if byteCounter == 0:\n",
    "                ETTT_arr[i]=TTT[i]\n",
    "                #ETTT_arr[i].append(TTT[i])\n",
    "                eventcount_arr[i]=(eventCounter[i])\n",
    "                byteCounter = 20\n",
    "            else:\n",
    "                ETTT_arr[i]=(data[i][byteCounter]+(data[i][byteCounter+1]<<16)+(data[i][byteCounter+2]<<32))\n",
    "                eventcount_arr[i]=(data[i][byteCounter+4]+(data[i][byteCounter+5]<<16))\n",
    "                byteCounter += 6\n",
    "            for j in range(0, numSamps[i]):\n",
    "                #if j == 0:\n",
    "                    #ys_arr[i].append([])\n",
    "                #print(byteCounter)\n",
    "                ys_arr[i][eventCount][j]=data[i][byteCounter]\n",
    "                byteCounter += 1\n",
    "            eventCount += 1\n",
    "    return ys_arr, ETTT_arr, eventcount_arr\n",
    "\n",
    "# start=time.time() \n",
    "# ys_arrHe, ETTT_arrHe, eventcount_arrHe  = dataread(read_data, [25], fileLength, numSamples) ##hardcoded channel 25 for He\n",
    "ys_arr, ETTT_arr, eventcount_arr  = dataread(read_data, chan_enab, fileLength, numSamples) ##hardcoded channels for coils\n",
    "\n",
    "end = time.time()\n",
    "print('dataread from binary time: ' + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeDif=[]\n",
    "for i in range(0,len(chan_enab)):\n",
    "    timeDif.append([])\n",
    "    for j in range(len(ETTT_arr[i])-1):\n",
    "        timeDif[i].append((ETTT_arr[i][j+1]-ETTT_arr[i][j])*8)\n",
    "#     print(\"Min time difference for channel\", chan_enab[i], \"is\", min(timeDif[i]), \"ns\")\n",
    "#     print(\"Max time difference for channel\", chan_enab[i], \"is\", max(timeDif[i]), \"ns \\n\")\n",
    "#print(timeDif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in SF and He normalization information ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     df_SF = pd.read_hdf(SFNormFile + '.h5', key='df_0')\n",
    "#     df_HE = pd.read_hdf(SFNormFile + '.h5', key='df_1')\n",
    "# except Exception as e:\n",
    "#     logger.error(run_num + ' failed during SFNormFile load')\n",
    "#     logger.exception(e)\n",
    "\n",
    "# SF_Sort_arr = df_SF[['nicknames', 'transition_locations']].to_numpy().T\n",
    "# He_Norm_arr = df_HE[['pulse', 'norms']].to_numpy().T\n",
    "\n",
    "# NormFactor = 1000000  ## He integrals are huge, this normalizes all of those by a constant value for ease of use\n",
    "# HeNorms= (He_Norm_arr[1])/NormFactor\n",
    "\n",
    "# # print(SF_Sort_arr) \n",
    "# # print(He_Norm_arr[1]/NormFactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting and/or base subtraction time: 17.246020555496216\n"
     ]
    }
   ],
   "source": [
    "# basesub and plotting ##\n",
    "\n",
    "baseL = 0\n",
    "baseR = int(((preTime[0]-groupStart[0])*0.70)/chanDec[0])  ##70% before the trigger\n",
    "numRuns = int((fileLength[0]-20-numSamples[0])/(numSamples[0]+6)+1)\n",
    "legend =  ['NaI', 'R']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "s = 20 ## pulse to look at \n",
    "t=s+1\n",
    "\n",
    "#  dont know why this is so slow ##\n",
    "def plotter(ys, xs, baseR, numpoints):\n",
    "    tempys_basesub = np.zeros((len(ys), numRuns,numpoints[0]), dtype=float)\n",
    "    for i in range((len(ys))):\n",
    "        for pulse in range((len(eventcount_arr[0]))): ## all have 5000 pulses\n",
    "            tempys_basesub[i][pulse]=np.subtract(ys[i][pulse], np.mean(ys[i][pulse][baseL:baseR]))\n",
    "        for j in range(s, t): ## plot only interested pulses\n",
    "            plt.plot(xs[i], tempys_basesub[i][j]) #label=legend[i]) #+str(sums[1][j])) ## sums[j] will not work for more than just TR   \n",
    "            plt.axvline(xs[0][baseL], ls = '--')\n",
    "            plt.axvline(xs[0][baseR], ls = '--')\n",
    "            #plt.axvline(xs[0][int(((preTime[0]-groupStart[0])*0.70)/chanDec[0])], ls = '--', c ='m')\n",
    "            plt.axvline(xs[0][baseR+5], ls = '--', c ='r') ## BaseR+5 line marks the beginning of the integral, until the end of samples.\n",
    "#             plt.title('SF state transition' + transitions[p]) \n",
    "#             plt.xlabel(\"time from trigger (ns)\")\n",
    "#             plt.ylabel(\"ADC\")\n",
    "#             plt.legend()\n",
    "            \n",
    "# plotter(ys_arr[9:], xs[9:], baseR, numSamples) ##plot coils\n",
    "\n",
    "ys_basesub = np.zeros((len(ys_arr), numRuns,numSamples[0]), dtype=np.float64)\n",
    "# ys_basesub_norm = np.zeros((len(ys_arr), numRuns,numSamples[0]), dtype=np.float64)\n",
    "\n",
    "## BASESUB ALSO CHANGES TO microQ NOW! 09.03.24\n",
    "@njit ## jit is faster for large # channels, slower for small # channels\n",
    "def basesub(ys, baseRight, numpoints):\n",
    "#     uQ_sec = ((2/4096)/50)*1000000 ## 4096 ADC = 2V, divide by 50Ohm to get I [Q/sec], change to microQ\n",
    "#     ys = ys*uQ_sec\n",
    "    tempys_basesub = np.zeros((numRuns,numpoints[0]), dtype=np.float64)\n",
    "    for pulse in range((len(eventcount_arr[0]))): ## all have 5000 pulses\n",
    "        tempys_basesub[pulse]=np.subtract(ys[pulse], np.mean(ys[pulse][baseL:baseRight]))\n",
    "    return tempys_basesub\n",
    "\n",
    "## BASESUB ALSO CHANGES TO microQ NOW! 09.03.24\n",
    "@njit ## jit is faster for large # channels, slower for small # channels\n",
    "def basesub_norm(ys, baseRight, numpoints): \n",
    "    tempys_basesub = np.zeros((numRuns,numpoints[0]), dtype=np.float64)\n",
    "#     uQ_sec = ((2/4096)/50)*1000000 ## 4096 ADC = 2V, divide by 50Ohm to get I [Q/sec], change to microQ\n",
    "#     ys = ys*uQ_sec\n",
    "    for pulse in range((len(eventcount_arr[0]))): ## all have 5000 pulses\n",
    "        tempys_basesub[pulse]=np.subtract(ys[pulse], np.mean(ys[pulse][baseL:baseRight]))\n",
    "        tempys_basesub[pulse]=tempys_basesub[pulse]/HeNorms[pulse] \n",
    "    return tempys_basesub\n",
    "\n",
    "# for peak finding algo., we don't want to use the normalization yet...\n",
    "for i in range(len(ys_basesub)): ## feeding y arrays into function 1 channel at  a time is faster than all at once\n",
    "    ys_basesub[i] = basesub(ys_arr[i], baseR, numSamples)\n",
    "# for i in range(len(ys_basesub)): ## if not using aligning/cutting later, ys should be normalized here\n",
    "#     ys_basesub[i] = basesub_norm(ys_arr[i], baseR, numSamples)\n",
    "\n",
    "ys_basesub[-1] = ys_basesub[-1]*-1 ## invert 6Li to positive signal. Comment out if not using\n",
    "# ys_basesub_norm[-1] = ys_basesub_norm[-1]*-1 ## invert 6Li to positive signal. Comment out if not using\n",
    "\n",
    "end = time.time()\n",
    "print('plotting and/or base subtraction time: ' + str(end-start))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aligning and cutting time: 9.977113962173462\n"
     ]
    }
   ],
   "source": [
    "# use 6Li t0 for all instead of for themselves individually ##\n",
    "NaIthresh=2000\n",
    "Li6thresh=1000\n",
    "\n",
    "threshold_array = (np.full(len(ys_basesub), NaIthresh))\n",
    "threshold_array[-1] = Li6thresh\n",
    "\n",
    "# njit ## numba does not support reversed, but this could be changed if it's slow\n",
    "\n",
    "def find_offset(ys, thresharr):\n",
    "    xCrosses = np.zeros((len(ys), numRuns)) #outer array is crossing arrays for given channel, inner array is crossing for each event\n",
    "    offset = np.zeros((len(ys), numRuns), dtype=np.int32) ##offset in bins for each channel, each pulse\n",
    "    modeCrosses = np.zeros((len(ys)), dtype=np.float64)\n",
    "    for i in reversed(range(len(ys))):\n",
    "        #xValues.append([])\n",
    "        for p in range(len(ys[i])):\n",
    "            xing = np.argmax(ys[i][p] > thresharr[i])\n",
    "            #print(xing)\n",
    "            xCrosses[i][p] = xing\n",
    "        modeCrosses[i] = (st.mode(xCrosses[i])) #find the most typical crossing value for each channel\n",
    "        for p in range(len(xCrosses[i])):\n",
    "            offset[i][p] = (modeCrosses[-1] - xCrosses[i][p]) ## make sure this is the correct sign!!! \n",
    "    if (np.all(xCrosses[-1])) == False:\n",
    "        emessage = ('ERROR: 6Li threshold was not reached for at least one pulse')\n",
    "        logger.error(run_num + emessage)\n",
    "        raise Exception(emessage)\n",
    "    return offset, xCrosses, modeCrosses\n",
    "                           \n",
    "offset, xCrosses, modeCrosses = find_offset(ys_basesub, threshold_array)\n",
    "\n",
    "end = time.time()\n",
    "# print('finding offset time: ' + str(end-start))  \n",
    "\n",
    "# extend all arrays by a value, check that the max number of offset on 6Li is less than that value ##\n",
    "start = time.time()\n",
    "\n",
    "extendedRange = 3 ## must be a positive value which to extend ys_arr\n",
    "if abs(max(offset[-1], key = abs)) > extendedRange: ## if the max offset of 6Li is >extendedRange, something is wrong\n",
    "    emessage = ('ERROR: largest offset greater than extended range')\n",
    "    logger.error(run_num + emessage)\n",
    "    raise Exception(emessage)\n",
    "\n",
    "try:\n",
    "    ys_ext = np.zeros((len(ys_basesub), len(ys_basesub[0]), len(ys_basesub[0][0])+extendedRange*2), dtype=np.float64)\n",
    "    ys_cut = np.zeros((len(ys_basesub), len(ys_basesub[0]), (len(ys_ext[0][0])-((extendedRange*2)+1)*2)))\n",
    "    xs_cut = np.zeros((len(ys_cut), len(ys_cut[0][0])))\n",
    "except Exception as e:\n",
    "    logger.error(run_num + ' failed during ys_cut array creation')\n",
    "    logger.exception(e)\n",
    "\n",
    "# cant use jit because np.pad is not supported ##\n",
    "\n",
    "def align_cut(ys, xs_arr, extendedr):\n",
    "    tempys_ext = np.zeros((len(ys), len(ys[0])+extendedr*2), dtype=np.float64)\n",
    "    tempys_cut = np.zeros((len(ys), (len(tempys_ext[0])-((extendedr*2)+1)*2)))\n",
    "    tempxs_cut = np.zeros(len(tempys_cut[0]))\n",
    "    for p in range(len(ys)):\n",
    "        tempys_ext[p] = np.pad(ys[p], extendedr, 'constant', constant_values=(0))\n",
    "        tempys_ext[p] = np.roll(tempys_ext[p],offset[-1][p]) ## assumes 6Li at -1 position\n",
    "        tempys_cut[p] = tempys_ext[p][((extendedr*2)+1):-((extendedr*2)+1)].copy() ## cut by 7 (if extRange == 3)\n",
    "#         tempys_cut[p] = tempys_cut[p]/HeNorms[p] ## normalize by 3He integral  ## comment out if using basesub_norm\n",
    "    x_cut_amt = int((len(ys[0]) - len(tempys_cut[0]))/2)\n",
    "    tempxs_cut = xs_arr[x_cut_amt:-x_cut_amt].copy()\n",
    "    return tempys_cut, tempxs_cut\n",
    "\n",
    "try:\n",
    "    for i in range(len(ys_basesub)):\n",
    "        ys_cut[i], xs_cut[i] = align_cut(ys_basesub[i], xs[i], extendedRange)\n",
    "except Exception as e:\n",
    "    logger.error(run_num + ' failed aligning and cutting')\n",
    "    logger.exception(e)\n",
    "    \n",
    "# checkp = 2053\n",
    "# print(offset[-1][checkp]) ## checking offset for one example checkpulse\n",
    "# print('original index for checkpulse: '+str(np.argmax(ys_basesub[0][checkp]> 2000))) ## we can follow the index as it changes with extension/cut\n",
    "# #print('extended range index for checkpulse: '+str(np.argmax(ys_ext[0][checkp]> 2000)))\n",
    "# print('cut array index for checkpulse: '+str(np.argmax((ys_cut[0][checkp]*HeNorms[checkp])> 2000)))\n",
    "\n",
    "del ys_ext ## might help with memory issues\n",
    "del ys_basesub\n",
    "\n",
    "end = time.time()\n",
    "print('aligning and cutting time: ' + str(end-start))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "# s = 342 ## pulse to look at \n",
    "# t=s+1\n",
    "# ch = 0\n",
    "    \n",
    "# leg =  ['NaI', 'R']\n",
    "# plt.legend(leg)\n",
    "# plt.title('One pulse') \n",
    "# plt.xlabel(\"time from trigger (ns)\")\n",
    "# plt.ylabel(\"ADC\")\n",
    "# # plt.ylabel(\"uC/sec\")\n",
    "\n",
    "# # ys_subset = ys_basesub[11][0][5000:9000].copy()\n",
    "# # xs_subset = xs[0][5000:9000].copy()\n",
    "# ys_subset = ys_basesub[ch][s].copy()\n",
    "# xs_subset = xs[ch].copy()\n",
    "\n",
    "# peaks, _ = sp.signal.find_peaks(ys_subset, height=0)\n",
    "# plt.plot(xs_subset, ys_subset)\n",
    "# # plt.plot(ys_subset[peaks],peaks, \"x\")\n",
    "# plt.axvline(xs[0][7600], ls = '--', c ='g')\n",
    "# plt.axvline(xs[0][8000], ls = '--', c ='g')\n",
    "# plt.axvline(xs[0][1500], ls = '--', c ='g')\n",
    "# plt.show()\n",
    "\n",
    "# plt.axvline(xs[0][baseL], ls = '--')\n",
    "# plt.axvline(xs[0][baseR], ls = '--')\n",
    "\n",
    "# plt.axvline(xs[1][intgrR], ls = '--', c ='g')\n",
    "# plt.axvline(xs[2][HeintgrL], ls = '--', c ='r')\n",
    "# plt.axvline(xs[2][HeintgrR], ls = '--', c ='r')\n",
    "\n",
    "# plotter(ys_basesub[10:], xs[10:], baseR, numSamples) ##plot coils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find peaks, integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak lengths are defined per pulse after being found; can maybe using numpy after that??\n",
    "# split into 2 functions to use JIT?\n",
    "\n",
    "## I think this has been confirmed to work using the singular finding method done earlier (below in troubleshooting).\n",
    "# @njit\n",
    "def find_peaks_2(ys, pkparam, peakrange): ## ys here is for one channel!\n",
    "#     pulse_arr = [[],[]] ## peaks index, pulse sum ranges\n",
    "    all_peaks = []\n",
    "    sum_ranges = []\n",
    "    ext = 40  ## how much to extend range by. Otherwise, can miss some peaks at the ends due to prominence\n",
    "    for p in range(len(ys)):  ## find peaks for every pulse p in one detector channel\n",
    "#         print(p)\n",
    "        p_sum_ranges = []\n",
    "        if peakrange[0] <= 0:\n",
    "            peaks, _ = sp.signal.find_peaks(ys[p][peakrange[0]:peakrange[1]+ext], threshold=pkparam[0], prominence=pkparam[1], height=pkparam[2])#, height  = [1,1000])\n",
    "        else:\n",
    "            peaks, _ = sp.signal.find_peaks(ys[p][peakrange[0]-ext:peakrange[1]], threshold=pkparam[0], prominence=pkparam[1], height=pkparam[2])#, height  = [1,1000])\n",
    "            peaks = peaks+peakrange[0]-ext\n",
    "#         print(peaks)\n",
    "        temp_i = np.array(np.where((peaks>=peakrange[0]+2) & (peaks<=peakrange[1]-2))[0], copy=True)\n",
    "        peaks = peaks[temp_i]\n",
    "        all_peaks.append(peaks)\n",
    "    return all_peaks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel 0\n",
      "channel 1\n",
      "channel 2\n",
      "channel 3\n",
      "channel 4\n",
      "channel 5\n",
      "channel 6\n",
      "channel 7\n",
      "channel 8\n",
      "channel 9\n",
      "channel 10\n",
      "channel 11\n",
      "finding peaks time: 15.044767379760742\n"
     ]
    }
   ],
   "source": [
    "# now try 2nd version\n",
    "\n",
    "start = time.time()\n",
    "peakthresh = 2 ## threshold for peaks is estimated\n",
    "prom = 10\n",
    "h = [0,750] \n",
    "param = [peakthresh, prom, h] ## all peak finding parameters\n",
    "\n",
    "grange = 1400\n",
    "peak_range_beg = [0, grange] ## hardcoded for 0-1400 and 7600-9000\n",
    "peak_range_end = [(len(ys_cut[0][0])-grange), len(ys_cut[0][0])]\n",
    "\n",
    "pks_beg = []\n",
    "pks_end = []\n",
    "\n",
    "for i in range(len(ys_cut[:-1])):\n",
    "    print('channel ' + str(chan_enab[i]))\n",
    "    pks_beg.append(find_peaks_2(ys_cut[i], param, peak_range_beg))\n",
    "    pks_end.append(find_peaks_2(ys_cut[i], param, peak_range_end))\n",
    "\n",
    "end = time.time()\n",
    "print('finding peaks time: ' + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabeo\\anaconda3\\lib\\site-packages\\numba\\core\\ir_utils.py:2152: NumbaPendingDeprecationWarning: \u001b[1m\n",
      "Encountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'peaks' of function 'integrate_peaks_jit_2'.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n",
      "\u001b[1m\n",
      "File \"C:\\Users\\gabeo\\AppData\\Local\\Temp\\ipykernel_34532\\3421468145.py\", line 2:\u001b[0m\n",
      "\u001b[1m@njit\n",
      "\u001b[1mdef integrate_peaks_jit_2(ys, peaks): ## ranges in which to integrate int_ranges = [[2,6], [6,10]], 2 len array for each point\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel 1\n",
      "channel 2\n",
      "channel 3\n",
      "channel 4\n",
      "channel 5\n",
      "channel 6\n",
      "channel 7\n",
      "channel 8\n",
      "channel 9\n",
      "channel 10\n",
      "channel 11\n",
      "integrating peaks time: 2.255478620529175\n"
     ]
    }
   ],
   "source": [
    "@njit\n",
    "def integrate_peaks_jit_2(ys, peaks): ## ranges in which to integrate int_ranges = [[2,6], [6,10]], 2 len array for each point\n",
    "    peak_integrals = List()\n",
    "    for p in range(len(peaks)):  ## find integral of peaks for every pulse p in one detector channel\n",
    "#         print(p)\n",
    "        peak_ints = np.zeros((len(peaks[p])), dtype = np.float64)\n",
    "#         peak_ints = [0]*(len(int_ranges[p]))\n",
    "#         print(p)\n",
    "#         print(len(peak_ints))\n",
    "        for pk in range(len(peaks[p])):  ## size of the peaks for every pulse\n",
    "            start_point = peaks[p][pk]-2 ## try this to condense code. Basically, the beginning of where to integrate\n",
    "            end_point = peaks[p][pk]+2\n",
    "#             print(end_point)\n",
    "            for point in range(start_point,end_point+1): ##From 20-60 for example. SFarr[2] is the array of start to end pulses to sum\n",
    "                peak_ints[pk] = np.add(peak_ints[pk],ys[p][point]) ## start with zeros, add to each iteratively\n",
    "        peak_integrals.append(peak_ints)\n",
    "    return peak_integrals\n",
    "    \n",
    "start = time.time()\n",
    "\n",
    "ints_beg_jit_2 =List()\n",
    "ints_end_jit_2 =List()\n",
    "\n",
    "for i in range(len(pks_beg)):\n",
    "    print('channel ' + str(chan_enab[i]))\n",
    "    ints_beg_jit_2.append(integrate_peaks_jit_2(ys_cut[i], pks_beg[i]))\n",
    "    ints_end_jit_2.append(integrate_peaks_jit_2(ys_cut[i], pks_end[i]))\n",
    "\n",
    "end = time.time()\n",
    "print('integrating peaks time: ' + str(end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.86785714  23.86785714  25.86785714  27.86785714 111.86785714\n",
      "  56.86785714  33.86785714  -6.13214286  29.86785714  -1.13214286]\n",
      "[124.86785714  97.86785714  52.86785714  19.86785714  26.86785714\n",
      "  34.86785714  29.86785714  38.86785714  42.86785714  49.86785714]\n",
      "\n",
      "[ 49.86785714 553.86785714 566.86785714  58.86785714 146.86785714\n",
      "  40.86785714  48.86785714 183.86785714  48.86785714  40.93      ]\n",
      "[ 15.86785714 129.86785714 141.86785714 141.86785714 150.86785714\n",
      " 178.86785714  42.86785714  19.86785714  20.86785714 114.86785714]\n"
     ]
    }
   ],
   "source": [
    "p = 1586\n",
    "\n",
    "print((ints_beg_jit_2[0][p][-10:]))\n",
    "# print((sum_ranges_beg_np[0][p][-10:]))\n",
    "print((ints_beg_jit_2[0][p][:10]))\n",
    "# print((sum_ranges_beg_np[0][p][-10:]))\n",
    "print()\n",
    "print((ints_end_jit_2[0][p][-10:]))\n",
    "# print((sum_ranges_beg_np[0][p][-10:]))\n",
    "print((ints_end_jit_2[0][p][:10]))\n",
    "# print((sum_ranges_beg_np[0][p][-10:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making histograms, changing to [uC]: 7.722012042999268\n"
     ]
    }
   ],
   "source": [
    "plt.ioff()\n",
    "start = time.time()\n",
    "Q_sec = ((2/4096)/50) ## 4096 ADC = 2V, divide by 50Ohm to get I [Q/sec]\n",
    "sec = (512*(10e-9)) ## 512 ns/bin here\n",
    "binnum = 500\n",
    "\n",
    "ints_beg_all = [] ## all pulse mode integrals for a given channel\n",
    "ints_end_all = []\n",
    "jit_np_beg = np.asarray(ints_beg_jit_2, dtype = object)\n",
    "jit_np_end = np.asarray(ints_end_jit_2, dtype = object)\n",
    "\n",
    "for i in range(len(ints_beg_jit_2)):\n",
    "    ints_beg_all.append(np.hstack(jit_np_beg[i]*Q_sec*sec))\n",
    "    ints_beg_all[i] = ints_beg_all[i]*10e6  ## convert to uC\n",
    "    ints_end_all.append(np.hstack(jit_np_end[i]*Q_sec*sec))\n",
    "    ints_end_all[i] = ints_end_all[i]*10e6\n",
    "    \n",
    "histdat_beg = [] ## all pulse mode histograms from the integrals\n",
    "histdat_end = []\n",
    "    \n",
    "for i in range(len(ints_beg_all)):\n",
    "    beghist = ints_beg_all[i]\n",
    "    endhist = ints_end_all[i]\n",
    "    beg_binval = plt.hist(beghist, bins = binnum, range = [0, 0.60]) ## this is a 2d array of bin y values and their bin locations\n",
    "    end_binval = plt.hist(endhist, bins = binnum, range = [0, 0.60])\n",
    "    histdat_beg.append(beg_binval)  ## 2d array of [[counts values], [bin locations (1 extra)]]\n",
    "    histdat_end.append(end_binval)\n",
    "\n",
    "end = time.time()\n",
    "print('making histograms, changing to [uC]: ' + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making histograms, changing to [uC]: 0.6846086978912354\n"
     ]
    }
   ],
   "source": [
    "## try np hist to see if it speeds up for bins = 400\n",
    "\n",
    "# plt.ioff()\n",
    "start = time.time()\n",
    "Q_sec = ((2/4096)/50) ## 4096 ADC = 2V, divide by 50Ohm to get I [Q/sec]\n",
    "sec = (512*(10e-9)) ## 512 ns/bin here\n",
    "binnum = 500\n",
    "\n",
    "ints_beg_all = [] ## all pulse mode integrals for a given channel\n",
    "ints_end_all = []\n",
    "jit_np_beg = np.asarray(ints_beg_jit_2, dtype = object)\n",
    "jit_np_end = np.asarray(ints_end_jit_2, dtype = object)\n",
    "\n",
    "for i in range(len(ints_beg_jit_2)):\n",
    "    ints_beg_all.append(np.hstack(jit_np_beg[i]*Q_sec*sec))\n",
    "    ints_beg_all[i] = ints_beg_all[i]*10e6  ## convert to uC\n",
    "    ints_end_all.append(np.hstack(jit_np_end[i]*Q_sec*sec))\n",
    "    ints_end_all[i] = ints_end_all[i]*10e6\n",
    "    \n",
    "histdat_beg = [] ## all pulse mode histograms from the integrals\n",
    "histdat_end = []\n",
    "    \n",
    "for i in range(len(ints_beg_all)):\n",
    "    beghist = ints_beg_all[i]\n",
    "    endhist = ints_end_all[i]\n",
    "    beg_binval = np.histogram(beghist, bins = binnum, range = [0, 0.60]) ## this is a 2d array of bin y values and their bin locations\n",
    "    end_binval = np.histogram(endhist, bins = binnum, range = [0, 0.60])\n",
    "    histdat_beg.append(beg_binval)  ## 2d array of [[counts values], [bin locations (1 extra)]]\n",
    "    histdat_end.append(end_binval)\n",
    "\n",
    "end = time.time()\n",
    "print('making histograms, changing to [uC]: ' + str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "## maybe include peak locations? but this would be a ch*pulse*# peaks sized array, very long\n",
    "\n",
    "cols = ['channel', 'begregion_integrals[uC]', 'endregion_integrals[uC]', 'begregion_hist[uC]', 'endregion_hist[uC]']\n",
    "intsData = [chan_enab[:-1], ints_beg_all, ints_end_all, histdat_beg, histdat_end]\n",
    "print(len(intsData))\n",
    "\n",
    "\n",
    "df_ints = pd.DataFrame({cols[0]: intsData[0],            \n",
    "                    cols[1]: intsData[1],\n",
    "                    cols[2]: intsData[2],\n",
    "                    cols[3]: intsData[3],\n",
    "                    cols[4]: intsData[4]})\n",
    "\n",
    "# print(df_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ints.to_hdf('F:/LANL/testhistogramsDF' + '.h5', f'df_0', mode='w') ## this \"deletes\" any previous data in the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma region analysis done, full time: 347.85343050956726\n",
      "finished 2024-09-16 18:22:38.886248\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullend = time.time()\n",
    "print('gamma region analysis done, full time: ' + str(fullend-fullstart))\n",
    "print('finished ' + str(datetime.now())) \n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end of data processing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "### testing end historgam - beg histogram\n",
    "%matplotlib qt\n",
    "\n",
    "testarr1 = df_ints['begregion_hist[uC]'].to_numpy()\n",
    "testarr2 = df_ints['endregion_hist[uC]'].to_numpy()\n",
    "# print(testarr1[0])\n",
    "# print(len(testarr1[0]))\n",
    "\n",
    "ch = 0\n",
    "\n",
    "# print(testarr2[0][0])\n",
    "testxbeg = testarr1[ch]\n",
    "testxend = testarr2[ch]\n",
    "testx = (testxend)\n",
    "\n",
    "print(len(testarr2[ch]))\n",
    "print(len(testx[0]))\n",
    "\n",
    "testend_beg = testxend[0]-testxbeg[0]\n",
    "plt.bar(testx[1][:-1], testend_beg, width = max(testx[1])/len(testx[0]))  ## this plots the previous histogram !!!\n",
    "plt.axvline(x = 0.0197, ls = '--', c = 'r', label = '511-662(??) keV peak from Cs spectrum')\n",
    "plt.legend()\n",
    "plt.xlabel('uC')\n",
    "plt.title('end-beg region pulse integrals run ' + run_num)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "2\n",
      "[-2.  0.  2.  4.  6.  8. 10. 12. 14.]\n"
     ]
    }
   ],
   "source": [
    "### testing end historgam - beg histogram\n",
    "## now roughly in MeV\n",
    "\n",
    "%matplotlib qt\n",
    "MeV_uC = 0.6617/0.03046875 # Cs peak in MeV\n",
    "\n",
    "ch = 0\n",
    "x = ints_beg_all[ch]*10e6*MeV_uC ## in microcoulomb\n",
    "print(type(x))\n",
    "ch2 =0\n",
    "x2 = ints_end_all[ch2]*10e6*MeV_uC\n",
    "# diff = x2-x  ## can't subtract these due to different num. of peaks and therefore integrals\n",
    "\n",
    "testarr1 = df_ints['begregion_hist[uC]'].to_numpy()\n",
    "testarr2 = df_ints['endregion_hist[uC]'].to_numpy()\n",
    "# print(testarr1[0])\n",
    "# print(len(testarr1[0]))\n",
    "\n",
    "ch = 0\n",
    "print(len(testarr2[ch]))\n",
    "\n",
    "# print(testarr2[0][0])\n",
    "testxbeg = testarr1[ch]\n",
    "testxend = testarr2[ch]\n",
    "testx = (testxend)\n",
    "\n",
    "testend_beg = testxend[0]-testxbeg[0]\n",
    "# plt.bar(testx[1][:-1]*keV_uC, testend_beg, width = max(testx[1])/len(testx[0]))  ## this plots the previous histogram !!!\n",
    "a = plt.bar(testx[1][:-1]*MeV_uC, testend_beg, width = max(testx[1])*MeV_uC/len(testx[0]))  ## this plots the previous histogram !!!\n",
    "\n",
    "# plt.axvline(x = 0.0197, ls = '--', c = 'r', label = '511-662(??) keV peak from Cs spectrum')\n",
    "plt.xlabel('MeV')\n",
    "plt.ylabel('Counts')\n",
    "# a.xticks([1,2,3,4,5,6,7,8])\n",
    "locs, labels = plt.xticks()\n",
    "plt.xticks(np.arange(0, 10, step=0.5))\n",
    "plt.minorticks_on()\n",
    "plt.axvline(x = 0.0197*MeV_uC, ls = '--', c = 'r', label = 'Avg Q from Cs spectrum')\n",
    "plt.axvline(x = 0.03046875*MeV_uC, ls = '--', c = 'y', label = '662 keV peak from Cs spectrum')\n",
    "plt.legend()\n",
    "print(locs)\n",
    "plt.title('end-beg pulse region integrals, run ' + run_num + ', chan '+str(chan_enab[ch]))\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
